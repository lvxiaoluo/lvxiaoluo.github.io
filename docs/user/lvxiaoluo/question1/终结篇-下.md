   - ### [**五、实战算法篇**](#五实战算法篇)
     - [**1、URL黑名单（布隆过滤器）**](#1url黑名单布隆过滤器)
     - [2、词频统计（分文件）**](#2词频统计分文件)
     - [3、未出现的数（bit数组）**](#3未出现的数bit数组)
     - [4、重复URL（分机器）**](#4重复url分机器)
     - [5、TOPK搜索（小根堆）**](#5topk搜索小根堆)
     - [6、中位数（单向二分查找）**](#6中位数单向二分查找)
     - [7、短域名系统（缓存）**](#7短域名系统缓存)
     - [8、海量评论入库（消息队列）**](#8海量评论入库消息队列)
     - [9、在线/并发用户数（Redis）**](#9在线并发用户数redis)
     - [10、热门字符串（前缀树）**](#10热门字符串前缀树)
     - [11、红包算法**](#11红包算法)
     - [11、手写快排**](#11手写快排)
     - [12、手写归并**](#12手写归并)
     - [13、手写堆排**](#13手写堆排)
     - [14、手写单例**](#14手写单例)
     - [15、手写LRUcache**](#15手写lrucache)
     - [16、手写线程池**](#16手写线程池)
     - [17、手写消费者生产者模式**](#17手写消费者生产者模式)
     - [18、手写阻塞队列**](#18手写阻塞队列)
     - [19、手写多线程交替打印ABC**](#19手写多线程交替打印abc)
     - [20、交替打印FooBar**](#20交替打印foobar)
  - ### [六、个人项目**](#六个人项目)
     - [**一、一站到底**](#一一站到底)
     - [1、如何设计排行榜**](#1如何设计排行榜) [**性能优化过程**](#性能优化过程)
     - [方案优化过程**](#方案优化过程)
     - [方案1：每日一个滚动榜，当日汇聚（费时间）**](#方案1每日一个滚动榜当日汇聚费时间)
     - [方案2：全局N个滚动榜同时写（费空间）**](#方案2全局n个滚动榜同时写费空间)
     - [方案3：实时更新，常数次写操作**](#方案3实时更新常数次写操作)
     - [2、如何解决重复答题**](#2如何解决重复答题)
     - [3、一个题目被多个人抢答**](#3一个题目被多个人抢答)
     - [4、如何管理昵称重复**](#4如何管理昵称重复)
     - [5、如何管理出题定时任务**](#5如何管理出题定时任务)
     - [6：如何解决客户端断连**](#6如何解决客户端断连)
     - [二、秒杀项目**](#二秒杀项目)
     - [技术选型**](#技术选型)
     - [方案设计**](#方案设计)
     - [1、如何解决超卖？**](#1如何解决超卖)
     - [2、如何解决重复下单？**](#2如何解决重复下单)
     - [3、如何防刷？**](#3如何防刷)
     - [4、热key问题如何解决？**](#4热key问题如何解决)
     - [5、应对高并发的读请求**](#5应对高并发的读请求)
     - [6、应对高并发的写请求**](#6应对高并发的写请求)
     - [7、如何保证数据一致性**](#7如何保证数据一致性)
     - [8、可靠性如何保障**](#8可靠性如何保障)
     - [9、秒杀系统瓶颈-日志**](#9秒杀系统瓶颈-日志)
     - [三、即时通信**](#三即时通信)
     - [1、单聊消息可靠传输**](#1单聊消息可靠传输)
     - [2、群聊消息如何保证不丢不重**](#2群聊消息如何保证不丢不重)
     - [3、如何保证消息的时序性**](#3如何保证消息的时序性)
     - [4、推拉结合**](#4推拉结合)
     - [5、好友推荐**](#5好友推荐)
     - [四、智慧社区**](#四智慧社区)
     - [物联网架构**](#物联网架构) [**DCM系统架构**](#dcm系统架构)
     - [三要素**](#三要素)
     - [云 / 边 / 端协同**](#云--边--端协同)
     - [物联网平台接入**](#物联网平台接入)
     - [门锁接入**](#门锁接入)
     - [各种协议**](#各种协议)
     - [IOT流量洪峰**](#iot流量洪峰)
     - [社区直播带货**](#社区直播带货) [**产品的背景**](#产品的背景)
     - [面临的挑战**](#面临的挑战)
     - [协议的比较**](#协议的比较)
     - [整体流程**](#整体流程)
     - [直播流程**](#直播流程)
     - [播放流程**](#播放流程)
     - [直播高可用方案**](#直播高可用方案)
     - [性能优化方案**](#性能优化方案)
     - [流量回放自动化测试**](#流量回放自动化测试)
  - ###[七、架构设计**](#七架构设计)
     - [**1、社区系统的架构**](#1社区系统的架构)
     - [2、商城系统-亿级商品如何存储**](#2商城系统-亿级商品如何存储)
     - [3、对账系统-分布式事务一致**性](#3对账系统-分布式事务一致性)
     - [4、用户系统-多线程数据割接**](#4用户系统-多线程数据割接)
     - [5、秒杀系统场景设计**](#5秒杀系统场景设计)
     - [6、统计系统-海量计数**](#6统计系统-海量计数)
     - [7、系统设计 - 微软**](#7系统设计---微软)
     - [1、需求收集**](#1需求收集)
     - [2、顶层设计**](#2顶层设计)
     - [3、系统核心指标**](#3系统核心指标)
     - [4、数据存储**](#4数据存储)
     - [7、如何设计一个微博**](#7如何设计一个微博)
  - ###[八、领域模型落地**](#八领域模型落地) 
     - [**1、拆分微服务**](#1拆分微服务)
     - [2、关联微服务**](#2关联微服务)
     - [3、微服务的落地**](#3微服务的落地)
     - [4、领域模型的意义**](#4领域模型的意义)
     - [5、战略建模**](#5战略建模)
     - [6、相关名词**](#6相关名词)


# 五、实战算法篇

### **1、URL黑名单（布隆过滤器）**

**100亿黑名单URL，每个64B，问这个黑名单要怎么存？判断一个URL是否在黑名单中**

 **散列表：**

 如果把黑名单看成一个集合，将其存在 hashmap 中，貌似太大了，需要 640G，明显不科学。

 **布隆过滤器：**

 它实际上是一个很长的二进制矢量和一系列随机映射函数。

 它**可以用来判断一个元素是否在一个集合中**。它的优势是只需要占用很小的内存空间以及有着高效的查询效率。对于布隆过滤器而言，它的本质是一个**位数组**：位数组就是数组的每个元素都只占用 1 bit ，并且每个元素只能是 0 或者 1。

 在数组中的每一位都是二进制位。布隆过滤器除了一个位数组，还有 K 个哈希函数。当一个元素加入布隆过滤器中的时候，会进行如下操作：

- 使用 K 个哈希函数对元素值进行 K 次计算，得到 K 个哈希值。
- 根据得到的哈希值，在位数组中把对应下标的值置为 1。

### **2、词频统计（分文件）**

**2GB内存在20亿整数中找到出现次数最多的数**

  通常做法是使用哈希表对出现的每一个数做词频统计，哈希表的key是某个整数，value记录整数出现的次数。本题的数据量是20亿，有可能一个数出现20亿次，则为了避免溢出，哈希表的key是32位（4B）,value也是 32位（4B），那么一条哈希表的记录就需要占用8B。

 当哈希表记录数为2亿个时，需要16亿个字节数（8*2亿），需要至少1.6GB内存(16亿/2^30,1GB== 2 ^30个字节 == 10亿)。则20亿个记录，至少需要16GB的内存，不符合题目要求。

  解决办法是将20亿个数的大文件利用哈希函数分成16个小文件，根据哈希函数可以把20亿条数据均匀分布到16个文件上，同一种数不可能被哈希函数分到不同的小文件上，假设哈希函数够好。然后对每一个小文件用哈希函数来统计其中每种数出现的次数，这样我们就得到16个文件中出现次数最多的数，接着从16个数中选出次数最大的那个key即可。

### **3、未出现的数**（bit数组）

**40亿个非负整数中找到没有出现的数**

  对于原问题，如果使用哈希表来保存出现过的数，那么最坏情况下是40亿个数都不相同，那么哈希表则需要保存40亿条数据，一个32位整数需要4B，那么40亿*4B = 160亿个字节，一般大概10亿个字节的数据需要1G的空间，那么大概需要16G的空间，这不符合要求。

　　我们换一种方式，申请一个bit数组，数组大小为4294967295，大概为40亿bit，40亿/8 = 5亿字节，那么需要0.5G空间，  bit数组的每个位置有两种状态0和1，那么怎么使用这个bit数组呢？呵呵，数组的长度刚好满足我们整数的个数范围，那么数组的每个下标值对应4294967295中的一个数，逐个遍历40亿个无符号数，例如，遇到20，则bitArray[20] = 1；遇到666，则bitArray[666] = 1,遍历完所有的数，将数组相应位置变为1。

**40亿个非负整数中找到一个没有出现的数，内存限制10MB**

  10亿个字节的数据大概需要1GB空间处理，那么10MB内存换算过来就是可以处理1千万字节的数据，也就是8千万bit，对于40亿非负整数如果申请bit数组的话，40亿bit / 0.8亿bit = 50，那么这样最少也得分50块来处理，下面就以64块来进行分析解答吧。

**总结一下进阶的解法：**

1．根据10MB的内存限制，确定统计区间的大小，就是第二次遍历时的bitArr大小。

2．利用区间计数的方式，找到那个计数不足的区间，这个区间上肯定有没出现的数。

3．对这个区间上的数做bit map映射，再遍历bit map，找到一个没出现的数即可。

**自己的想法**

如果只是找一个数，可以高位模运算，写到64个不同的文件，然后在最小的文件中通过bitArray一次处理掉。

**40亿个无符号整数，1GB内存，找到所有出现两次的数**

 对于原问题，可以用bit  map的方式来表示数出现的情况。具体地说，是申请一个长度为4294967295×2的bit类型的数组bitArr，用2个位置表示一个数出现的词频，1B占用8个bit，所以长度为4294967295×2的bit类型的数组占用1GB空间。怎么使用这个bitArr数组呢？遍历这40亿个无符号数，如果初次遇到num，就把bitArr[num*2 +  1]和bitArr[num*2]设置为01，如果第二次遇到num，就把bitArr[num*2+1]和bitArr[num*2]设置为10，如果第三次遇到num，就把bitArr[num*2+1]和bitArr[num*2]设置为11。以后再遇到num，发现此时bitArr[num*2+1]和bitArr[num*2]已经被设置为11，就不再做任何设置。遍历完成后，再依次遍历bitArr，如果发现bitArr[i*2+1]和bitArr[i*2]设置为10，那么i 就是出现了两次的数。

### **4、重复URL**（分机器）

**找到100亿个URL中重复的URL**

  原问题的解法使用解决大数据问题的一种常规方法：把大文件通过哈希函数分配到机器，或者通过哈希函数把大文件拆成小文件。一直进行这种划分，直到划分的结果满足资源限制的要求。首先，你要向面试官询问在资源上的限制有哪些，包括内存、计算时间等要求。在明确了限制要求之后，可以将每条URL通过哈希函数分配到若干机器或者拆分成若干小文件，这里的“若干”由具体的资源限制来计算出精确的数量。

 例如，将100亿字节的大文件通过哈希函数分配到100台机器上，然后每一台机器分别统计分给自己的URL中是否有重复的URL，**同时哈希函数的性质决定了同一条URL不可能分给不同的机器；**或者在单机上将大文件通过哈希函数拆成1000个小文件，对每一个小文件再利用哈希表遍历，找出重复的URL；或者在分给机器或拆完文件之后，进行排序，排序过后再看是否有重复的URL出现。总之，牢记一点，很多大数据问题都离不开分流，要么是哈希函数把大文件的内容分配给不同的机器，要么是哈希函数把大文件拆成小文件，然后处理每一个小数量的集合。

### **5、TOPK搜索（小根堆）**

**海量搜索词汇，找到最热TOP100词汇的方法**

  最开始还是用哈希分流的思路来处理，把包含百亿数据量的词汇文件分流到不同的机器上，具体多少台机器由面试官规定或者由更多的限制来决定。对每一台机器来说，如果分到的数据量依然很大，比如，内存不够或其他问题，可以再用哈希函数把每台机器的分流文件拆成更小的文件处理。

  处理每一个小文件的时候，哈希表统计每种词及其词频，哈希表记录建立完成后，再遍历哈希表，遍历哈希表的过程中使用大小为100的小根堆来选出每一个小文件的top 100（整体未排序的top 100）。每一个小文件都有自己词频的小根堆（整体未排序的top  100），将小根堆里的词按照词频排序，就得到了每个小文件的排序后top 100。然后把各个小文件排序后的top  100进行外排序或者继续利用小根堆，就可以选出每台机器上的top  100。不同机器之间的top100再进行外排序或者继续利用小根堆，最终求出整个百亿数据量中的top 100。对于top K  的问题，除哈希函数分流和用哈希表做词频统计之外，还经常用堆结构和外排序的手段进行处理。

### **6、中位数（单向二分查找）**

**10MB内存，找到100亿整数的中位数**

①内存够：内存够还慌什么啊，直接把100亿个全部排序了，你用冒泡都可以...然后找到中间那个就可以了。但是你以为面试官会给你内存？？

②内存不够：题目说是整数，我们认为是带符号的int,所以4字节，占32位。

假设100亿个数字保存在一个大文件中，依次读一部分文件到内存(不超过内存的限制)，将每个数字用二进制表示，比较二进制的最高位(第32位，符号位，0是正，1是负)，如果数字的最高位为0，则将这个数字写入 file_0文件中；如果最高位为 1，则将该数字写入file_1文件中。

从而将100亿个数字分成了两个文件，假设 file_0文件中有 60亿 个数字，file_1文件中有 40亿 个数字。那么中位数就在 file_0 文件中，并且是 file_0  文件中所有数字排序之后的第 10亿  个数字。（file_1中的数都是负数，file_0中的数都是正数，也即这里一共只有40亿个负数，那么排序之后的第50亿个数一定位于file_0中）

现在，我们只需要处理 file_0 文件了（不需要再考虑file_1文件）。对于 file_0  文件，同样采取上面的措施处理：将file_0文件依次读一部分到内存(不超内存限制)，将每个数字用二进制表示，比较二进制的  次高位（第31位），如果数字的次高位为0，写入file_0_0文件中；如果次高位为1，写入file_0_1文件 中。

现假设 file_0_0文件中有30亿个数字，file_0_1中也有30亿个数字，则中位数就是：file_0_0文件中的数字从小到大排序之后的第10亿个数字。

抛弃file_0_1文件，继续对 file_0_0文件 根据 次次高位(第30位)  划分，假设此次划分的两个文件为：file_0_0_0中有5亿个数字，file_0_0_1中有25亿个数字，那么中位数就是  file_0_0_1文件中的所有数字排序之后的 第 5亿 个数。

按照上述思路，直到划分的文件可直接加载进内存时，就可以直接对数字进行快速排序，找出中位数了。

### **7、短域名系统（缓存）**

**设计短域名系统，将长URL转化成短的URL.**

（1）利用放号器，初始值为0，对于每一个短链接生成请求，都递增放号器的值，再将此值转换为62进制（a-zA-Z0-9），比如第一次请求时放号器的值为0，对应62进制为a，第二次请求时放号器的值为1，对应62进制为b，第10001次请求时放号器的值为10000，对应62进制为sBc。

（2）将短链接服务器域名与放号器的62进制值进行字符串连接，即为短链接的URL，比如：[t.cn/sBc。](http://t.cn/sBc。)

（3）重定向过程：生成短链接之后，需要存储短链接到长链接的映射关系，即sBc -> URL，浏览器访问短链接服务器时，根据URL Path取到原始的链接，然后进行302重定向。映射关系可使用K-V存储，比如Redis或Memcache。

### **8、海量评论入库（消息队列）**

**假设有这么一个场景，有一条新闻，新闻的评论量可能很大，如何设计评论的读和写**

前端页面直接给用户展示、通过消息队列异步方式入库

读可以进行读写分离、同时热点评论定时加载到缓存

### **9、在线/并发用户数（Redis）**

 **显示网站的用户在线数的解决思路**

 维护在线用户表

 使用Redis统计

**显示网站并发用户数**

1. 每当用户访问服务时，把该用户的 ID 写入ZSORT队列，权重为当前时间
2. 根据权重(即时间)计算一分钟内该机构的用户数Zrange
3. 删掉一分钟以上过期的用户Zrem

### 10、热门字符串（前缀树）

假设目前有 1000w 个记录（这些查询串的重复度比较高，虽然总数是 1000w，但如果除去重复后，则不超过 300w 个）。请统计最热门的 10 个查询串，要求使用的内存不能超过 1G。（一个查询串的重复度越高，说明查询它的用户越多，也就越热门。）

**HashMap 法**

虽然字符串总数比较多，但去重后不超过 300w，因此，可以考虑把所有字符串及出现次数保存在一个 HashMap 中，所占用的空间为 300w*(255+4)≈777M（其中，4 表示整数占用的 4 个字节）。由此可见，1G 的内存空间完全够用。

**思路如下**：

首先，遍历字符串，若不在 map 中，直接存入 map，value 记为 1；若在 map 中，则把对应的 value 加 1，这一步时间复杂度 `O(N)` 。

接着遍历 map，构建一个 10 个元素的小顶堆，若遍历到的字符串的出现次数大于堆顶字符串的出现次数，则进行替换，并将堆调整为小顶堆。

遍历结束后，堆中 10 个字符串就是出现次数最多的字符串。这一步时间复杂度 `O(Nlog10)` 。

**前缀树法**

当这些字符串有大量相同前缀时，可以考虑使用前缀树来统计字符串出现的次数，树的结点保存字符串出现次数，0 表示没有出现。

**思路如下**：

在遍历字符串时，在前缀树中查找，如果找到，则把结点中保存的字符串次数加 1，否则为这个字符串构建新结点，构建完成后把叶子结点中字符串的出现次数置为 1。

最后依然使用**小顶堆**来对字符串的出现次数进行排序。

### 11、红包算法

线性切割法，一个区间切N-1刀。越早越多

二倍均值法，【0 ~ 剩余金额 / 剩余人数 * 2】中随机，相对均匀

![img](https://tva1.sinaimg.cn/large/008eGmZEly1goqpbvl5pvj30qu0gcgm0.jpg)

![img](https://tva1.sinaimg.cn/large/008eGmZEly1goqpc3hz9dj31450ggq8k.jpg)

### 11、手写快排

```java
public class QuickSort {
    public static void swap(int[] arr, int i, int j) {
        int tmp = arr[i];
        arr[i] = arr[j];
        arr[j] = tmp;
    }    /* 常规快排 */

    public static void quickSort1(int[] arr, int L, int R) {
        if (L > R) return;
        int M = partition(arr, L, R);
        quickSort1(arr, L, M - 1);
        quickSort1(arr, M + 1, R);
    }

    public static int partition(int[] arr, int L, int R) {
        if (L > R) return -1;
        if (L == R) return L;
        int lessEqual = L - 1;
        int index = L;
        while (index < R) {
            if (arr[index] <= arr[R]) swap(arr, index, ++lessEqual);
            index++;
        }
        swap(arr, ++lessEqual, R);
        return lessEqual;
    }    /* 荷兰国旗 */

    public static void quickSort2(int[] arr, int L, int R) {
        if (L > R) return;
        int[] equalArea = netherlandsFlag(arr, L, R);
        quickSort2(arr, L, equalArea[0] - 1);
        quickSort2(arr, equalArea[1] + 1, R);
    }

    public static int[] netherlandsFlag(int[] arr, int L, int R) {
        if (L > R) return new int[]{-1, -1};
        if (L == R) return new int[]{L, R};
        int less = L - 1;
        int more = R;
        int index = L;
        while (index < more) {
            if (arr[index] == arr[R]) {
                index++;
            } else if (arr[index] < arr[R]) {
                swap(arr, index++, ++less);
            } else {
                swap(arr, index, --more);
            }
        }
        swap(arr, more, R);
        return new int[]{less + 1, more};
    }    // for test

    public static void main(String[] args) {
        int testTime = 1;
        int maxSize = 10000000;
        int maxValue = 100000;
        boolean succeed = true;
        long T1 = 0, T2 = 0;
        for (int i = 0; i < testTime; i++) {
            int[] arr1 = generateRandomArray(maxSize, maxValue);
            int[] arr2 = copyArray(arr1);
            int[] arr3 = copyArray(arr1);
            //          int[] arr1 = {9,8,7,6,5,4,3,2,1};
            long t1 = System.currentTimeMillis();
            quickSort1(arr1, 0, arr1.length - 1);
            long t2 = System.currentTimeMillis();
            quickSort2(arr2, 0, arr2.length - 1);
            long t3 = System.currentTimeMillis();
            T1 += (t2 - t1);
            T2 += (t3 - t2);
            if (!isEqual(arr1, arr2) || !isEqual(arr2, arr3)) {
                succeed = false;
                break;
            }
        }
        System.out.println(T1 + " " + T2);
        //      System.out.println(succeed ? "Nice!" : "Oops!");    }   
        private static int[] generateRandomArray(int maxSize, int maxValue){
            int[] arr = new int[(int) ((maxSize + 1) * Math.random())];
            for (int i = 0; i < arr.length; i++) {
                arr[i] = (int) ((maxValue + 1) * Math.random()) - (int) (maxValue * Math.random());
            }
            return arr;
        }
        private static int[] copyArray ( int[] arr){
            if (arr == null) return null;
            int[] res = new int[arr.length];
            for (int i = 0; i < arr.length; i++) {
                res[i] = arr[i];
            }
            return res;
        }
        private static boolean isEqual ( int[] arr1, int[] arr2){
            if ((arr1 == null && arr2 != null) || (arr1 != null && arr2 == null)) return false;
            if (arr1 == null && arr2 == null) return true;
            if (arr1.length != arr2.length) return false;
            for (int i = 0; i < arr1.length; i++) if (arr1[i] != arr2[i]) return false;
            return true;
        }
        private static void printArray ( int[] arr){
            if (arr == null) return;
            for (int i = 0; i < arr.length; i++) System.out.print(arr[i] + " ");
            System.out.println();
        }
    }
}
```

### 12、手写归并

```java
    public static void merge(int[] arr, int L, int M, int R) {
        int[] help = new int[R - L + 1];
        int i = 0;
        int p1 = L;
        int p2 = M + 1;
        while (p1 <= M && p2 <= R) help[i++] = arr[p1] <= arr[p2] ? arr[p1++] : arr[p2++];
        while (p1 <= M) help[i++] = arr[p1++];
        while (p2 <= R) help[i++] = arr[p2++];
        for (i = 0; i < help.length; i++) arr[L + i] = help[i];
    }

    public static void mergeSort(int[] arr, int L, int R) {
        if (L == R) return;
        int mid = L + ((R - L) >> 1);
        process(arr, L, mid);
        process(arr, mid + 1, R);
        merge(arr, L, mid, R);
    }

    public static void main(String[] args) {
        int[] arr1 = {9, 8, 7, 6, 5, 4, 3, 2, 1};
        mergeSort(arr, 0, arr.length - 1);
        printArray(arr);
    }
```
### 13、手写堆排

```java
 // 堆排序额外空间复杂度O(1)
    public static void heapSort(int[] arr) {
        if (arr == null || arr.length < 2) return;
        for (int i = arr.length - 1; i >= 0; i--) heapify(arr, i, arr.length);
        int heapSize = arr.length;
        swap(arr, 0, --heapSize);
        // O(N*logN)
        while (heapSize > 0) { // O(N)
            heapify(arr, 0, heapSize); // O(logN)
            swap(arr, 0, --heapSize); // O(1)
        }
    }

    // arr[index]刚来的数，往上
    public static void heapInsert(int[] arr, int index) {
        while (arr[index] > arr[(index - 1) / 2]) {
            swap(arr, index, (index - 1) / 2);
            index = (index - 1) / 2;
        }
    }

    // arr[index]位置的数，能否往下移动
    public static void heapify(int[] arr, int index, int heapSize) {
        int left = index * 2 + 1; // 左孩子的下标
        while (left < heapSize) { // 下方还有孩子的时候
            // 两个孩子中，谁的值大，把下标给largest
            // 1）只有左孩子，left -> largest
            // 2) 同时有左孩子和右孩子，右孩子的值<= 左孩子的值，left -> largest
            // 3) 同时有左孩子和右孩子并且右孩子的值> 左孩子的值， right -> largest
            int largest = left + 1 < heapSize && arr[left + 1] > arr[left] ? left + 1 : left;
            // 父和较大的孩子之间，谁的值大，把下标给largest        
            largest = arr[largest] > arr[index] ? largest : index;
            if (largest == index) break;
            swap(arr, largest, index);
            index = largest;
            left = index * 2 + 1;
        }
    }

    public static void swap(int[] arr, int i, int j) {
        int tmp = arr[i];
        arr[i] = arr[j];
        arr[j] = tmp;
    }

    public static void main(String[] args) {
        int[] arr1 = {9, 8, 7, 6, 5, 4, 3, 2, 1};
        heapSort(arr1);
        printArray(arr1);
    }
```
### 14、手写单例

```java
public class Singleton {
    private volatile static Singleton singleton;

    private Singleton() {
    }

    public static Singleton getSingleton() {
        if (singleton == null) {
            synchronized (Singleton.class) {
                if (singleton == null) {
                    singleton = new Singleton();
                }
            }
        }
        return singleton;
    }
}
```

### 15、手写LRUcache

```java
// 基于linkedHashMap
public class LRUCache {
    private LinkedHashMap<Integer, Integer> cache;
    private int capacity;   //容量大小

    public LRUCache(int capacity) {
        cache = new LinkedHashMap<>(capacity);
        this.capacity = capacity;
    }

    public int get(int key) {
        //缓存中不存在此key，直接返回
        if (!cache.containsKey(key)) {
            return -1;
        }
        int res = cache.get(key);
        cache.remove(key);   //先从链表中删除
        cache.put(key, res);  //再把该节点放到链表末尾处
        return res;
    }

    public void put(int key, int value) {
        if (cache.containsKey(key)) {
            cache.remove(key); //已经存在，在当前链表移除
        }
        if (capacity == cache.size()) {
            //cache已满，删除链表头位置
            Set<Integer> keySet = cache.keySet();
            Iterator<Integer> iterator = keySet.iterator();
            cache.remove(iterator.next());
        }
        cache.put(key, value);  //插入到链表末尾   
    }
}
```
```java
//手写双向链表
class LRUCache {
    class DNode {
        DNode prev;
        DNode next;
        int val;
        int key;
    }

    Map<Integer, DNode> map = new HashMap<>();
    DNode head, tail;
    int cap;

    public LRUCache(int capacity) {
        head = new DNode();
        tail = new DNode();
        head.next = tail;
        tail.prev = head;
        cap = capacity;
    }

    public int get(int key) {
        if (map.containsKey(key)) {
            DNode node = map.get(key);
            removeNode(node);
            addToHead(node);
            return node.val;
        } else {
            return -1;
        }
    }

    public void put(int key, int value) {
        if (map.containsKey(key)) {
            DNode node = map.get(key);
            node.val = value;
            removeNode(node);
            addToHead(node);
        } else {
            DNode newNode = new DNode();
            newNode.val = value;
            newNode.key = key;
            addToHead(newNode);
            map.put(key, newNode);
            if (map.size() > cap) {
                map.remove(tail.prev.key);
                removeNode(tail.prev);
            }
        }
    }

    public void removeNode(DNode node) {
        DNode prevNode = node.prev;
        DNode nextNode = node.next;
        prevNode.next = nextNode;
        nextNode.prev = prevNode;
    }

    public void addToHead(DNode node) {
        DNode firstNode = head.next;
        head.next = node;
        node.prev = head;
        node.next = firstNode;
        firstNode.prev = node;
    }
}
```

### **16、手写线程池**

```java
package com.concurrent.pool;

import java.util.HashSet;
import java.util.Set;
import java.util.concurrent.ArrayBlockingQueue;
import java.util.concurrent.BlockingQueue;

public class MySelfThreadPool {
    //默认线程池中的线程的数量
    private static final int WORK_NUM = 5;
    //默认处理任务的数量
    private static final int TASK_NUM = 100;
    private int workNum;// 线程数量
    private int taskNum;//任务数量
    private final Set<WorkThread> workThreads;//保存线程的集合
    private final BlockingQueue<Runnable> taskQueue;//阻塞有序队列存放任务

    public MySelfThreadPool() {
        this(WORK_NUM, TASK_NUM);
    }

    public MySelfThreadPool(int workNum, int taskNum) {
        if (workNum <= 0) workNum = WORK_NUM;
        if (taskNum <= 0) taskNum = TASK_NUM;
        taskQueue = new ArrayBlockingQueue<>(taskNum);
        this.workNum = workNum;
        this.taskNum = taskNum;
        workThreads = new HashSet<>();
        //启动一定数量的线程数，从队列中获取任务处理
        for (int i = 0; i < workNum; i++) {
            WorkThread workThread = new WorkThread("thead_" + i);
            workThread.start();
            workThreads.add(workThread);
        }
    }

    public void execute(Runnable task) {
        try {
            taskQueue.put(task);
        } catch (InterruptedException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }
    }

    public void destroy() {
        System.out.println("ready close thread pool...");
        if (workThreads == null || workThreads.isEmpty()) return;
        for (WorkThread workThread : workThreads) {
            workThread.stopWork();
            workThread = null;//help gc
        }
        workThreads.clear();
    }

    private class WorkThread extends Thread {
        public WorkThread(String name) {
            super();
            setName(name);
        }

        @Override
        public void run() {
            while (!interrupted()) {
                try {
                    Runnable runnable = taskQueue.take();//获取任务
                    if (runnable != null) {
                        System.out.println(getName() + " readyexecute:" + runnable.toString());
                        runnable.run();//执行任务
                    }
                    runnable = null;//help gc
                } catch (Exception e) {
                    interrupt();
                    e.printStackTrace();
                }
            }
        }

        public void stopWork() {
            interrupt();
        }
    }
}
package com.concurrent.pool;

public class TestMySelfThreadPool {
    private static final int TASK_NUM = 50;//任务的个数

    public static void main(String[] args) {
        MySelfThreadPool myPool = new MySelfThreadPool(3, 50);
        for (int i = 0; i < TASK_NUM; i++) {
            myPool.execute(new MyTask("task_" + i));
        }
    }

    static class MyTask implements Runnable {
        private String name;

        public MyTask(String name) {
            this.name = name;
        }

        public String getName() {
            return name;
        }

        public void setName(String name) {
            this.name = name;
        }

        @Override
        public void run() {
            try {
                Thread.sleep(1000);
            } catch (InterruptedException e) {
                // TODO Auto-generated catch block
                e.printStackTrace();
            }
            System.out.println("task :" + name + " end...");
        }

        @Override
        public String toString() {
            // TODO Auto-generated method stub
            return "name = " + name;
        }
    }
}
```

### **17、手写消费者生产者模式**

```java
public class Storage {
    private static int MAX_VALUE = 100;
    private List<Object> list = new ArrayList<>();

    public void produce(int num) {
        synchronized (list) {
            while (list.size() + num > MAX_VALUE) {
                System.out.println("暂时不能执行生产任务");
                try {
                    list.wait();
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }
            for (int i = 0; i < num; i++) {
                list.add(new Object());
            }
            System.out.println("已生产产品数" + num + " 仓库容量" + list.size());
            list.notifyAll();
        }
    }

    public void consume(int num) {
        synchronized (list) {
            while (list.size() < num) {
                System.out.println("暂时不能执行消费任务");
                try {
                    list.wait();
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }
            for (int i = 0; i < num; i++) {
                list.remove(0);
            }
            System.out.println("已消费产品数" + num + " 仓库容量" + list.size());
            list.notifyAll();
        }
    }
}

public class Producer extends Thread {
    private int num;
    private Storage storage;

    public Producer(Storage storage) {
        this.storage = storage;
    }

    public void setNum(int num) {
        this.num = num;
    }

    public void run() {
        storage.produce(this.num);
    }
}

public class Customer extends Thread {
    private int num;
    private Storage storage;

    public Customer(Storage storage) {
        this.storage = storage;
    }

    public void setNum(int num) {
        this.num = num;
    }

    public void run() {
        storage.consume(this.num);
    }
}

public class Test {
    public static void main(String[] args) {
        Storage storage = new Storage();
        Producer p1 = new Producer(storage);
        Producer p2 = new Producer(storage);
        Producer p3 = new Producer(storage);
        Producer p4 = new Producer(storage);
        Customer c1 = new Customer(storage);
        Customer c2 = new Customer(storage);
        Customer c3 = new Customer(storage);
        p1.setNum(10);
        p2.setNum(20);
        p3.setNum(80);
        c1.setNum(50);
        c2.setNum(20);
        c3.setNum(20);
        c1.start();
        c2.start();
        c3.start();
        p1.start();
        p2.start();
        p3.start();
    }
}
```

### **18、手写阻塞队列**

```java
public class blockQueue {
    private List<Integer> container = new ArrayList<>();
    private volatile int size;
    private volatile int capacity;
    private Lock lock = new ReentrantLock();
    private final Condition isNull = lock.newCondition();
    private final Condition isFull = lock.newCondition();

    blockQueue(int capacity) {
        this.capacity = capacity;
    }

    public void add(int data) {
        try {
            lock.lock();
            try {
                while (size >= capacity) {
                    System.out.println("阻塞队列满了");
                    isFull.await();
                }
            } catch (Exception e) {
                isFull.signal();
                e.printStackTrace();
            }
            ++size;
            container.add(data);
            isNull.signal();
        } finally {
            lock.unlock();
        }
    }

    public int take() {
        try {
            lock.lock();
            try {
                while (size == 0) {
                    System.out.println("阻塞队列空了");
                    isNull.await();
                }
            } catch (Exception e) {
                isNull.signal();
                e.printStackTrace();
            }
            --size;
            int res = container.get(0);
            container.remove(0);
            isFull.signal();
            return res;
        } finally {
            lock.unlock();
        }
    }
}

    public static void main(String[] args) {
        AxinBlockQueue queue = new AxinBlockQueue(5);
        Thread t1 = new Thread(() -> {
            for (int i = 0; i < 100; i++) {
                queue.add(i);
                System.out.println("塞入" + i);
                try {
                    Thread.sleep(500);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }
        });
        Thread t2 = new Thread(() -> {
            for (; ; ) {
                System.out.println("消费" + queue.take());
                try {
                    Thread.sleep(800);
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }
        });
        t1.start();
        t2.start();
    }
}
```

### **19、手写多线程交替打印ABC**

```java
package com.demo.test;

import java.util.concurrent.locks.Condition;
import java.util.concurrent.locks.ReentrantLock;

public class syncPrinter implements Runnable {
    // 打印次数
    private static final int PRINT_COUNT = 10;
    private final ReentrantLock reentrantLock;
    private final Condition thisCondtion;
    private final Condition nextCondtion;
    private final char printChar;

    public syncPrinter(ReentrantLock reentrantLock, Condition thisCondtion, Condition nextCondition, char printChar) {
        this.reentrantLock = reentrantLock;
        this.nextCondtion = nextCondition;
        this.thisCondtion = thisCondtion;
        this.printChar = printChar;
    }

    @Override
    public void run() {
        // 获取打印锁 进入临界区
        /reentrantLock.lock();
        try {
            // 连续打印PRINT_COUNT次
            for (int i = 0; i < PRINT_COUNT; i++) {                //打印字符
                System.out.print(printChar);
                // 使用nextCondition唤醒下一个线程
                // 因为只有一个线程在等待，所以signal或者signalAll都可以
                nextCondtion.signal();
                // 不是最后一次则通过thisCondtion等待被唤醒
                // 必须要加判断，不然虽然能够打印10次，但10次后就会直接死锁
                if (i < PRINT_COUNT - 1) {
                    try {
                        // 本线程让出锁并等待唤醒
                        thisCondtion.await();
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                }
            }
        } finally {
            reentrantLock.unlock();
        }
    }

    public static void main(String[] args) throws InterruptedException {
        ReentrantLock lock = new ReentrantLock();
        Condition conditionA = lock.newCondition();
        Condition conditionB = lock.newCondition();
        Condition conditionC = lock.newCondition();
        Thread printA = new Thread(new syncPrinter(lock, conditionA, conditionB, 'A'));
        Thread printB = new Thread(new syncPrinter(lock, conditionB, conditionC, 'B'));
        Thread printC = new Thread(new syncPrinter(lock, conditionC, conditionA, 'C'));
        printA.start();
        Thread.sleep(100);
        printB.start();
        Thread.sleep(100);
        printC.start();
    }
}
```

### 20、交替打印FooBar

```java
//手太阴肺经 BLOCKING Queue
public class FooBar {
    private int n;
    private BlockingQueue<Integer> bar = new LinkedBlockingQueue<>(1);
    private BlockingQueue<Integer> foo = new LinkedBlockingQueue<>(1);

    public FooBar(int n) {
        this.n = n;
    }

    public void foo(Runnable printFoo) throws InterruptedException {
        for (int i = 0; i < n; i++) {
            foo.put(i);
            printFoo.run();
            bar.put(i);
        }
    }

    public void bar(Runnable printBar) throws InterruptedException {
        for (int i = 0; i < n; i++) {
            bar.take();
            printBar.run();
            foo.take();
        }
    }
}

//手阳明大肠经CyclicBarrier 控制先后
class FooBar6 {
    private int n;

    public FooBar6(int n) {
        this.n = n;
    }

    CyclicBarrier cb = new CyclicBarrier(2);
    volatile boolean fin = true;

    public void foo(Runnable printFoo) throws InterruptedException {
        for (int i = 0; i < n; i++) {
            while (!fin) ;
            printFoo.run();
            fin = false;
            try {
                cb.await();
            } catch (BrokenBarrierException e) {
            }
        }
    }

    public void bar(Runnable printBar) throws InterruptedException {
        for (int i = 0; i < n; i++) {
            try {
                cb.await();
            } catch (BrokenBarrierException e) {
            }
            printBar.run();
            fin = true;
        }
    }
}

//手少阴心经 自旋 + 让出CPU
class FooBar5 {
    private int n;

    public FooBar5(int n) {
        this.n = n;
    }

    volatile boolean permitFoo = true;

    public void foo(Runnable printFoo) throws InterruptedException {
        for (int i = 0; i < n; ) {
            if (permitFoo) {
                printFoo.run();
                i++;
                permitFoo = false;
            } else {
                Thread.yield();
            }
        }
    }

    public void bar(Runnable printBar) throws InterruptedException {
        for (int i = 0; i < n; ) {
            if (!permitFoo) {
                printBar.run();
                i++;
                permitFoo = true;
            } else {
                Thread.yield();
            }
        }
    }
}

//手少阳三焦经 可重入锁 + Condition
class FooBar4 {
    private int n;

    public FooBar4(int n) {
        this.n = n;
    }

    Lock lock = new ReentrantLock(true);
    private final Condition foo = lock.newCondition();
    volatile boolean flag = true;

    public void foo(Runnable printFoo) throws InterruptedException {
        for (int i = 0; i < n; i++) {
            lock.lock();
            try {
                while (!flag) {
                    foo.await();
                }
                printFoo.run();
                flag = false;
                foo.signal();
            } finally {
                lock.unlock();
            }
        }
    }

    public void bar(Runnable printBar) throws InterruptedException {
        for (int i = 0; i < n; i++) {
            lock.lock();
            try {
                while (flag) {
                    foo.await();
                }
                printBar.run();
                flag = true;
                foo.signal();
            } finally {
                lock.unlock();
            }
        }
    }
}

//手厥阴心包经 synchronized + 标志位 + 唤醒
class FooBar3 {
    private int n;    // 标志位，控制执行顺序，true执行printFoo，false执行printBar
    private volatile boolean type = true;
    private final Object foo = new Object(); // 锁标志

    public FooBar3(int n) {
        this.n = n;
    }

    public void foo(Runnable printFoo) throws InterruptedException {
        for (int i = 0; i < n; i++) {
            synchronized (foo) {
                while (!type) {
                    foo.wait();
                }
                printFoo.run();
                type = false;
                foo.notifyAll();
            }
        }
    }

    public void bar(Runnable printBar) throws InterruptedException {
        for (int i = 0; i < n; i++) {
            synchronized (foo) {
                while (type) {
                    foo.wait();
                }
                printBar.run();
                type = true;
                foo.notifyAll();
            }
        }
    }
}

//手太阳小肠经 信号量 适合控制顺序
class FooBar2 {
    private int n;
    private Semaphore foo = new Semaphore(1);
    private Semaphore bar = new Semaphore(0);

    public FooBar2(int n) {
        this.n = n;
    }

    public void foo(Runnable printFoo) throws InterruptedException {
        for (int i = 0; i < n; i++) {
            foo.acquire();
            printFoo.run();
            bar.release();
        }
    }

    public void bar(Runnable printBar) throws InterruptedException {
        for (int i = 0; i < n; i++) {
            bar.acquire();
            printBar.run();
            foo.release();
        }
    }
}
```

# **六、个人项目**

## **一、一站到底**

 采用SpringBoot构建项目，主要通过分布式缓存、队列、限流保证系统高可用，Netty、缓存、反向代理保证高并发。

> 双人对战答题、公司对战抢答

### 1、如何设计排行榜

- 个人总得分和总排名实时更新
- 个人排行榜按分数、时间、次数、正确率展示
- 日榜、过去N日榜滚动更新

#### 性能优化过程

 第一条需求很简单，使用了Redis的**Zset**实现不过这里总得分采用了基于**分数、时间、次数和正确率**的混合加权。考虑到数据的**持久化**，以及**关系数据库和缓存的一致性**导致的设计的复杂性，使用了**谷歌**开源的**JamsRanking**

 优点**是可以直接使用现成的setScores和getRanking接口封装了Redis和Mysql和消息队列的完成**事务和一致性**的使用细节。缺点是**并发比较低**使用Jmeter进行压测，单机只有\**20**左右的**TPS**

 后来看了下源码，主要是它针对每一次设置都进行了分布式事务处理，并且会返回事务提交或回滚的结果。了解了底层实现以后就去谷歌的**开源社区**去查阅了相关的解决方案，当时官方对这个问题并没有通过**配置能直接解决问题**的快捷方式，不过推荐了使用者自身如果对响应时间不高的场景下可以采用**批量合并事务**的方式进行优化。基于这个思路，我们把写操作进行了封装并放入了**队列**，然后在消费者端批量取得数据后进行事务的批量处理，压测环境下整体性能达到了**500TPS**。已经基本满足了线上更新的需求，但是当时压测的过程中，队列偶尔的吞吐量会**大范围波动**，经常会持续数十秒，然后业务一次性处理完再响应，导致**局部响应时间大幅度增长**

 后来也是在官网上查询，了解到谷歌开源组件使用的**队列服务**底层是使用**BigTable**作为持久层，但是当BigTable分片过大时，会触发**再分片**的过程，再分片的过程中，是**不会进行任务分发**的，所以就会导致先前的问题。针对这个问题，谷歌官方的建议是提前**配置队列的数量、负载策略和最大容量**等信息，保证所有队列**不同时触发**再分片

 进行两次优化后，压测环境已经基本可以满足预期了，在实际生产环境的部署中，发现对于事务更新失败时，JamsRanking会对失败的事务进行**切分和重试**，整个过程对于研发人员是**透明**的，不利于线上问题排查，所以我们当时特地写了一个watchdog的工具，监控事务回滚达到十次以上的事务，查明原因后通过后台管理系统进行相应补偿，保证**最终一致性**

**最终结果：**

- 高效快速：能在数百毫秒内找到玩家排名以及进行更新
- 强一致性以及持久化、排名准确
- 可以扩展到任意数量的玩家
- 吞吐量有限制，只能支持约每秒 500次更新。

针对这个缺点谷歌官方也是给出了使用分片树和近似排名的解决方案，当然复杂的方案有更高的运维成本，所以我们优化工作也就到此为止

#### 方案优化过程

#### 方案1：每日一个滚动榜，当日汇聚（费时间）

 首先记录每天的排行榜和一个滚动榜，加分时同时写入这两个榜单，每日零点后跑工具将前几天数据累加写入当日滚动榜，该方案缺点是时间复杂度高，7天榜还好，只需要读过去6天数据，如果是100天榜，该方案需要读过去99天榜，显然不可接受

#### 方案2：全局N个滚动榜同时写（费空间）

  要做到每日零点后榜单实时生效，而不需要等待离线作业的完成，一种方案是预写未来的榜单。可以写当天的滚动榜的同时，写往后N-1天的滚动榜一起写入该方案不仅能脱离离线作业做到实时更新，且可以省略每天的日榜。但缺点也不难看出，对于7天滚动榜，每次写操作需要更新7个榜单，但是对于百日榜，空间消耗无法接受，1000万榜单大约消耗1G内存

#### 方案3：实时更新，常数次写操作

有不有办法做到既能实时更新，写榜数量也不随N的增加而增加呢？

  仍然是记录每天的排行榜和一个滚动榜，加分操作也还是同时操作当日榜和全局榜，但每日零点的离线作业改为从全局榜中减去之前过期的数据，从而实现先滚动更新。 此方案每次只需读取一个日榜做减法，时间复杂度为O(1)；但是无法做到实时更新。  这个方案的优点是在十二点前提前准备好差分榜，到了十二点直接加上当天数据就是滚动榜内容 ，这样就在常数次写操作的前提下，实现了滚动榜的实时更新

### 2、**如何解决重复答题**

 **利用setnx防止重复答题**
​ 分布式锁是控制分布式系统之间同步访问共享资源的一种方式。 利用Redis的单线程特性对共享资源进行串行化处理

```
// 获取锁推荐使用set的方式String result = jedis.set(lockKey, requestId, "NX", "EX", expireTime);
// 推荐使用redis+lua脚本String lua = "if redis.call('get',KEYS[1]) == ARGV[1] then return redis.call('del',KEYS[1]) else return 0 end";Object result = jedis.eval(lua, Collections.singletonList(lockKey)
```

### **3、一个题目被多个人抢答**

 **利用redis来实现乐观锁（抢答）**，好处是答错的人不影响状态，第一个秒杀答对的人才能得分。

1、利用redis的watch功能，监控这个 Corp:Activ:Qust: 的状态值
2、获取Corp:Activ:Qust: 的值，创建redis事务，给这个key的值-1
3、执行这个事务，如果key的值被修改过则回滚，key不变

### **4、如何管理昵称重复**

 **使用布隆过滤器：**

 它实际上是一个很长的二进制矢量数组和 K 个哈希函数。当一个昵称加入布隆过滤器中的时候，会进行如下操作：

- 使用 K 个哈希函数对元素值进行 K 次计算，得到 K 个哈希值。
- 根据得到的哈希值，在位数组中把对应下标的值置为 1。 Na

 用户新增昵称时需要首先计算K个哈希值，如果K个哈希值有一个不为0则通过，否则不通过，不通过时通过加随机字符串再次检验，检测通过后返回给前端，帮助用户自动填写。

 布隆过滤器的好处是它**可以用来判断一个元素是否在一个集合中**。它的优势是只需要占用很小的内存空间以及有着高效的查询效率。对于布隆过滤器而言，它的本质是一个**位数组**：位数组就是数组的每个元素都只占用 1 bit ，并且每个元素只能是 0 或者 1。

BloomFilter 的优势是，全内存操作，性能很高。另外空间效率非常高，**要达到 1% 的误判率，平均单条记录占用 1.2 字节即可。而且，平均单条记录每增加 0.6 字节，还可让误判率继续变为之前的  1/10，即平均单条记录占用 1.8 字节，误判率可以达到 1/1000；平均单条记录占用 2.4 字节，误判率可以到 1/10000，以此类推**。这里的误判率是指，BloomFilter 判断某个 key 存在，但它实际不存在的概率，因为它存的是 key 的 Hash 值，而非 key 的值，所以有概率存在这样的  key，它们内容不同，但多次 Hash 后的 Hash 值都相同。对于 BloomFilter 判断不存在的 key ，则是 100%  不存在的，反证法，如果这个 key 存在，那它每次 Hash 后对应的 Hash 值位置肯定是 1，而不会是 0



### **5、如何管理出题定时任务**

 压测环境中服务器通过Netty的主从Reactor多路复用NIO事件模型，单机可以**轻松应对十万长连接**，但是每个业务中，由于每个用户登录系统后需要按照指定顺序答题，例如一共要答十道，那么服务器针对这一个用户就会产生十个定时任务，所以对于系统来说，定时器的**数量就是百万级别的**。

 通过压测结果发现：JDK自带的Timer，在大概三万并发时性能就急剧下降了。也是此时根据业务场景的需要，将定时任务改成了Netty自带的HashedWheelTimer时间轮方案，通过压测单机在50万级别下依然能够平滑的执行。

 也是这个强烈的反差，使我在强烈的好奇心促使下，阅读源码了解到常规的JDK 的Timer 和 DelayedQueue 等工具类，可实现简单的定时任务，单底层用的是**堆数据结构**，存取复杂度都是 **O(NlogN)**，无法支撑海量定时任务。**Netty经典的时间轮方案**，正是通过将任务存取及取消操作时间复杂度降为 O(1)，而广泛应用在定时**任务量大、性能要求高**的场景中。



 基于Netty的Websocket底层，服务器端维护一个高效批量管理定时任务的调度模型。时间轮一般会实现成一个**环形数组结构**，类似一个时钟，分为很多槽，一个槽代表一个时间间隔，每个槽使用**双向链表**存储定时任务。指针**周期性地跳动**，跳动到一个槽位，就执行该槽位的定时任务。

 单层时间轮的容量和精度都是有限的，对于精度要求特别高、时间跨度特别大或是海量定时任务需要调度的场景，可以考虑使用多级时间轮以及持久化存储与时间轮结合的方案。时间轮的**定时任务处理逻辑**如下：

1. 将缓存在 timeouts 队列中的定时任务转移到时间轮中对应的槽中
2. 根据当前指针定位对应槽，处理该槽位的双向链表中的定时任务，从链表头部开始迭代：
   - 属于当前时钟周期则取出运行
   - 不属于则将其剩余的时钟周期数减一
3. 检测时间轮的状态。如果时间轮处于运行状态，则循环执行上述步骤，不断执行定时任务。

### **6：如何解决客户端断连**

 使用Netty的**重连检测狗**ConnectionWatchdog

 服务端定义refreshTime，当我们从channel中read到了服务端发来的心跳响应消息的话，就刷新refreshTime为当前时间

 客户端在state是WRITER_IDLE的时候每隔一秒就发送一个心跳包到sever端，告诉server端我还活着。

当重连成功时，会触发channelActive方法，在这里我们开启了一个定时任务去判断refreshTime和当前时间的时间差，超过5秒说明断线了，要进行重连，最后计算重连次数，尝试连接2次以上连不上就会修改header信息强制重连去连另一个服务器。

## 二、秒杀项目

### **技术选型**

秒杀用到的基础组件，主要有**框架、KV 存储、关系型数据库、MQ**。

框架主要有 Web 框架和 RPC 框架。

其中，Web 框架主要用于提供 HTTP 接口给浏览器访问，所以 Web 框架的选型在秒杀服务中非常重要。在这里，我**推荐Gin**，它的性能和易用性都不错，在 **GitHub 上的 Star 达到了 44k**。对比性能最好的 fasthttp，虽然 fasthttp 在请求延迟低于 10ms 时性能优势明显，但其底层使用的对象池容易让人踩坑，导致其易用性较差，所以没必要过于追求性能而忽略了稳定性

至于 RPC 框架，我推荐选用 **gRPC**，因为它的扩展性和性能都非常不错。在秒杀系统中，Redis 中的数据主要是给秒杀接口服务使用，以便将配置从管理后台同步到 Redis 缓存中。

KV 存储方面，秒杀系统中主要是用 **Redis 缓存活动配置**，用 **etcd 存储集群信息**。

关系型数据库中，**MySQL** 技术成熟且稳定可靠，秒杀系统用它存储活动配置数据很合适。主要 原因还是秒杀活动信息和库存数据都缓存在 Redis 中，活动过程中秒杀服务不操作数据库， 使用 MySQL 完全能够满足需求。

MQ 有很多种，其中 **Kafka** 在业界认可度最高，技术也非常成熟，性能很不错，非常适合用在秒杀系统中。Kafka 支持自动创建队列，秒杀服务各个节点可以用它自动创建属于自己的队列

### 方案设计

**背景**

## - 秒杀业务简单，每个秒杀活动的商品是事先定义好的，商品有明确的类型和数量，卖完即止

秒杀活动定时上架，消费者可以在活动开始后，通过秒杀入口进行抢购秒杀活动

- 
  秒杀活动由于商品物美价廉，开始售卖后，会被快速抢购一空。

**现象**

- 秒杀活动持续时间短，访问冲击量大，秒杀系统需要应对这种爆发性的访问模型
- 业务的请求量远远大于售卖量，大部分是陪跑的请求，秒杀系统需要提前规划好处理策略
- 前端访问量巨大，系统对后端数据的访问量也会短时间爆增，对数据存储资源进行良好设计
- 活动期间会给整个业务系统带来超大负荷，需要制定各种策略，避免系统过载而宕机
- 售卖活动商品价格低廉，存在套利空间，各种非法作弊手段层出，需要提前规划预防策略

**秒杀系统设计**

 首先，要**尽力将请求拦截在系统上游**，层层设阻拦截，过滤掉无效或超量的请求。因为访问量远远大于商品数量，所有的请求打到后端服务的最后一步，其实并没有必要，反而会严重拖慢真正能成交的请求，降低用户体验。

 秒杀系统专为秒杀活动服务，售卖商品确定，因此可以在设计秒杀商品页面时，将商品信息提前设计为静态信息，将静态的商品信息以及常规的 CSS、JS、宣传图片等静态资源，一起**独立存放到 CDN 节点**，加速访问，且降低系统访问压力，在访问前端也可以**制定种种限制策略，**比如活动没开始时，抢购按钮置灰，避免抢先访问，用户抢购一次后，也将按钮置灰，让用户排队等待，避免反复刷新。

 其次，要**充分利用缓存**，提升系统的性能和可用性。

 用户所有的请求进入秒杀系统前，通过**负载均衡策略**均匀分发到不同 Web 服务器，避免节点过载。在 Web 服务器中，首先检查用户的访问权限，识别并发刷订单的行为。如果发现售出数量已经达到秒杀数量，则直接返回结束，要将秒杀业务系统和其他业务系统进行功能分拆，尽量将秒杀系统及依赖服务**独立分拆部署**，避免影响其他核心业务系统。

 秒杀系统需要构建访问记录缓存，记录访问 IP、用户的访问行为，发现异常访问，提前进行阻断及返回。同时还需要**构建用户缓存**，并针对历史数据分析，提前缓存僵尸强刷专业户，方便在秒杀期间对其进行策略限制。这些访问记录、用户数据，通过缓存进行存储，可以加速访问，另外，对用户数据还进行缓存预热，避免活动期间大量穿透。

### **1、如何解决超卖？**

mysql乐观锁+redis预减库存+redis缓存卖完标记

第一是基于**数据库乐观锁**的方式保证数据并发扣减的强一致性；

第二是基于**数据库的事务**实现批量扣减部分失败时的数据回滚。

 在扣减指定数量前应先做一次前置数量校验的读请求（参考**读写分离** + **全缓存方案**）

> 纯数据库乐观锁+事务的方式性能比较差，但是如果不计成本和考虑场景的话也完全够用，因为任何没有机器配置的指标，都是耍流氓。如果我采用 Oracle 的数据库、100 多核的刀锋服务器、SSD 的硬盘，即使是纯数据库的扣减方案，也是可以达到单机上万的 TPS 的。

**单线程Redis 的 lua 脚本实现批量扣减**

当用户调用扣减接口时，将扣减的 对应数量 + 脚本标示传递至 Redis 即可，所有的扣减判断逻辑均在 Redis 中的 lua 脚本中执行，lua 脚本执行完成之后返还是否成功给客户端。



Redis 中的 lua 脚本执行时，首先会使用 get 命令查询 uuid 进行查重。当防重通过后，会**批量获取对应的剩余库存状态并进行判断**，如果一个扣减的数量大于剩余数量，则返回错误并提示数量不足。

Redis 的单线程模型，确保**不会出现当所有扣减数量在判断均满足后，在实际扣减时却数量不够**。同时，单线程保证判断数量的步骤和后续扣减步骤之间，没有其他任何线程出现并发的执行。

当 Redis 扣减成功后，扣减接口会**异步的将此次扣减内容保存至数据库**。异步保存数据库的目的是防止出现极端情况—— Redis 宕机后数据未持久化到磁盘，此时我们可以使用数据库恢复或者校准数据

最后，运营后台直连数据库，是运营和商家修改库存的入口。商家在运营后台进货物进行补充。同时，运营后台的实现需要将此数量**同步的增加至 Redis**，因为当前方案的所有实际扣减都在 Redis 中

> 纯缓存方案虽**不会导致超卖**，但因**缓存不具备事务特性**，极端情况下会存在缓存里的数据**无法回滚**，导致出现**少卖**的情况。且架构中的异步写库，也可能发生失败，导致多扣的数据丢失

可以借助**顺序写**的特性，将扣减任务同步**插入**任务表，发现异常时，将任务表作为**undolog**进行回滚

可以解决由于**网络不通**、调用缓存**扣减超时**、在扣减到一半时缓存**突然宕机**（故障 failover）了。针对上述请求，都有相应的异常抛出，根据异常进行**数据库回滚**即可，最终任务库里的数据都是准的

更进一步：由于任务库是无状态的，可以进行水平分库，提升整体性能

### **2、如何解决重复下单？**

mysql唯一索引+分布式锁

### **3、如何防刷？**

IP限流 | 验证码 | 单用户 | 单设备 | IMEI | 源IP |均设置规则

### **4、热key问题如何解决？**

redis集群+本地缓存+限流+key加随机值分布在多个实例中

1、**缓存集群**可以单节点进行**主从复制和垂直扩容**

2、利用应用内的**前置缓存**，但是需注意需要设置上限

3、延迟不敏感，**定时刷新**，实时感知用主动刷新

4、和缓存穿透一样，限制逃逸流量，单请求进行数据**回源并刷新前置**

5、无论如何设计，最后都要写一个**兜底逻辑**，千万级流量说来就来

### **5、应对高并发的读请求**

使用缓存策略将请求挡在上层中的缓存中

使用CDN，能静态化的数据尽量做到静态化，

加入限流（比如对短时间之内来自某一个用户，某一个IP、某个设备的重复请求做丢弃处理）

**资源隔离限流**会将对应的资源按照指定的类型进行隔离，比如**线程池**和**信号量**。

- 计数器限流，例如5秒内技术1000请求，超数后限流，未超数重新计数

- 滑动窗口限流，解决计数器不够精确的问题，把一个窗口拆分多滚动窗口

- 令牌桶限流，类似景区售票，售票的速度是固定的，拿到令牌才能去处理请求

- 漏桶限流，生产者消费者模型，实现了恒定速度处理请求，能够绝对防止突发流量

  流量控制效果从好到差依次是：**漏桶限流 > 令牌桶限流 > 滑动窗口限流 > 计数器限流**

  

  其中，只有漏桶算法**真正实现了恒定速度处理请求**，能够绝对**防止突发流量超过下游系统承载能力**。
  不过，漏桶限流也有个不足，就是需要分**配内存资源缓存请求**，这会增加内存的使用率。而**令牌桶限流**算法中的“桶”可以用一个整数表示，**资源占用相对较小**，这也让它成为最常用的限流算法。正是因为这些特点，**漏桶限流和令牌桶限流**经常在一些大流量系统中结合使用。

### **6、应对高并发的写请求**

- **削峰**：恶意用户拦截

  对于单用户多次点击、单设备、IMEI、源IP均设置规则

- 采用比较成熟的**漏桶算法、令牌桶**算法，也可以使用**guava**开箱即用的限流算法

  可以集群限流，但单机限流更加简洁和稳定

- 当前层**直接过滤**一定比例的请求，最大承载值前需要加上**兜底逻辑**

- 对于已经无货的产品，**本地缓存**直接返回

- **单独部署，减少对系统正常服务的影响，方便扩缩容**

对于**一段时间内的秒杀活动，需要保证写成功**，我们可以使用 **消息队列**。

- 削去秒杀场景下的峰值写流量——**流量削峰**
- 通过异步处理简化秒杀请求中的业务流程——**异步处理**
- 解耦，实现秒杀系统模块之间松耦合——**解耦**

**削去秒杀场景下的峰值写流量**

- **将秒杀请求暂存于息队列**，业务服务器响应用户“秒杀结果正在处理中。。。”，释放系统资源去处理其它用户的请求。
- **削峰填谷**，削平短暂的流量高峰，消息堆积会造成请求延迟处理，但秒杀用户对于短暂延迟有一定容忍度。秒杀商品有 1000 件，处理一次购买请求的时间是 500ms，那么总共就需要 500s 的时间。这时你部署 10  个队列处理程序，那么秒杀请求的处理时间就是 50s，也就是说用户需要等待 50s 才可以看到秒杀的结果，这是可以接受的。这时会**并发 10 个**请求到达数据库，并不会对数据库造成很大的压力。

**通过异步处理简化秒杀请求中的业务流程**

 先处理主要的业务，异步处理次要的业务。

- 如主要流程是**生成订单**、**扣减库存**；
- 次要流程比如购买成功之后会给用户**发优惠券**，**增加用户的积\****分**。
- 此时秒杀只要处理生成订单，扣减库存的耗时，发放优惠券、增加用户积分异步去处理了。

**解耦**

 实现秒杀系统模块之间松耦合将秒杀数据同步给数据团队，有两种思路：

- 使用 HTTP 或者 RPC 同步调用，即提供一个接口，实时将数据推送给数据服务。**系统的耦合度高**，如果其中一个服务有问题，可能会导致另一个服务不可用。
- 使用消息队列**将数据全部发送给消息队列**，然后**数据服务订阅这个消息队列**，接收数据进行处理。

### **7、如何保证数据一致性**

**CacheAside旁路缓存**读请求不命中查询数据库，查询完成写入缓存，写请求更新数据库后删除缓存数据。

```
// 延迟双删，用以保证最终一致性,防止小概率旧数据读请求在第一次删除后更新数据库public void write(String key,Object data){    redis.delKey(key);    db.updateData(data);    Thread.sleep(1000);    redis.delKey(key);}
```

为防缓存失效这一信息丢失，可用消息队列确保。

- 更新数据库数据；
- 数据库会将操作信息写入binlog日志当中；
- 另起一段非业务代码，程序订阅提取出所需要的数据以及key；
- 尝试删除缓存操作，若删除失败，将这些信息发送至消息队列；
- 重新从消息队列中获得该数据，重试操作；

订阅**binlog程序在mysql中有现成的中间**件叫canal，试机制，主要采用的是消息队列的方式。

**终极方案：请求串行化**

真正靠谱非秒杀的方案：将访问操作串行化

1. 先删缓存，将更新数据库的**写操作放进有序队列中**
2. 从缓存查不到的**读操作也进入有序队列**

需要解决的问题：

1. 读请求积压，大量超时，导致数据库的压力：限流、熔断
2. 如何避免大量请求积压：将队列水平拆分，提高并行度。

### 8、可靠性如何保障**

 由一个或多个sentinel实例组成sentinel集群可以监视一个或多个主服务器和多个从服务器。**哨兵模式适合读请求远多于写请求的业务场景，比如在秒杀系统**中用来缓存活动信息。 如果写请求较多，当集群 Slave 节点数量多了后，Master 节点同步数据的压力会非常大。



 当主服务器进入下线状态时，sentinel可以将该主服务器下的某一从服务器升级为主服务器继续提供服务，从而保证redis的高可用性。

### 9、秒杀系统瓶颈-日志

> 秒杀服务单节点需要处理的请求 QPS 可能达到 10 万以上。一个请求从进入秒杀服务到处理失败或者成功，至少会产生两条日志。也就是说，高峰期间，一个秒杀节点每秒产生的日志可能达到 **30 万条**以上

 一块性能比较好的固态硬盘，每秒写的IOPS 大概在 3 万左右。也就是说，一个秒杀节点的每秒日志条数是固态硬盘 IOPS 的 10 倍，磁盘都扛不住，更别说通过网络写入到监控系统中。

- **每秒日志量远高于磁盘 IOPS**，直接写磁盘会影响服务性能和稳定性
- 大量日志导致服务频繁分配，**频繁释放内存，影响服务性能**。
- 服务异常退出**丢失大量日志**的问题

**解决方案**

- **Tmpfs**，即临时文件系统，它是一种基于内存的文件系统。我们可以将秒杀服务写日志的文件放在临时文件系统中。相比直接写磁盘，在临时文件统中写日志的性能至少**能提升 100 倍**，每当日志文件达到 20MB 的时候，就将**日志文件转移到磁盘上**，并将临时文件系统中的日志文件清空。
- 可以参考内存池设计，将给logger分配缓冲区，每一次的新写可以复用Logger对象
- 参考kafka的缓冲池设计，当缓冲区达到大小和间隔时长临界值时，调用Flush函数，减少丢失的风险

**10、池化技术**

![image-20210504174220668](https://tva1.sinaimg.cn/large/008i3skNly1gq6japwof2j31520na4mh.jpg)

 通常可以采用**循环队列**来保存空闲连接。使用的时候，可以从队列头部取出连接，用完后将空闲连接放到队列尾部。Netty中利用带缓冲区的 channel 来充当队列。

## 三、即时通信

### 1、**单聊消息可靠传输**

TCP保证消息可靠传输三板斧：超时、重传、确认。服务端和客户端通信MSG和ACK的共计6个报文

- 请求报文（request，后简称为为R），客户端主动发送给服务端。
- 应答报文（acknowledge，后简称为A），服务器被动应答客户端的报文。
- 通知报文（notify，后简称为N），服务器主动发送给客户端的报文

**在线消息流程：**

 A 消息请求 **MSG:R** => S 消息应答 **MSG:A** => S 消息通知B **MSG:N**

 S 确认通知 **ACK:N** <= S 确认应答 **ACK:A** <= B确认请求S **ACK:R**

**超时与重传、确认和去重：**

 A发出了 **MSG:R** ，收到了**MSG:A**之后，在一个期待的时间内，如果没有收到**ACK:N**，A会尝试将 **MSG:R** 重发。可能A同时发出了很多消息，所以A需要在本地维护一个等待ack队列，并配合timer超时机制，来记录哪些消息没有收到**ACK:N**，定时重发。确认ACK**保证必达**，去重保证**唯一**

**离线消息流程**

 原方案：根据离线好友的标识，交互拉取指定的消息

![IM消息送达保证机制实现(二)：保证离线消息的可靠投递_2.png](https://tva1.sinaimg.cn/large/0081Kckwly1gm8kxci29zj30b305974z.jpg)

优化的方案：

- 如用户**勾选全量**则返回计数，在用户点击时拉取。
- 如用户未勾选全量则返回**最近全部离线消息**，客户端针对**用户id进行计算**。
- 全量离线信息可以通过客户端异步线程分页拉取，减少卡顿
- 将ACK和分页第二次拉取的报文重合，可以较少离线消息拉取交互的次数

### **2、群聊消息如何保证不丢不重**

> 在线的群友能第一时间收到消息；
> 离线的群友能在登陆后收到消息。

![IM群聊消息如此复杂，如何保证不丢不重？_1.jpg](https://tva1.sinaimg.cn/large/0081Kckwly1gm8jswr3poj30hh078dg2.jpg)

- 群消息发送者x向server发出群消息；
- server去db中查询群中有多少用户(x,A,B,C,D)；
- server去cache中查询这些用户的在线状态；
- 对于群中在线的用户A与B，群消息server进行实时推送；
- 对于群中离线的用户C与D，群消息server进行离线存储。

 对于同一份群消息的内容，多个离线用户存储了很多份。假设群中有200个用户离线，离线消息则冗余了200份，这极大的增加了数据库的存储压力

- 离线消息表只存储用户的群离线消息msg_id，降低数据库的冗余存储量
- 加入应用层的ACK，才能保证群消息一定到达，服务端幂等性校验及客户端去重，保证不重复
- 每条群消息都ACK，会给服务器造成巨大的冲击，通过批量ACK减少消息风暴扩散系数的影响
- 群离线消息过多：拉取过慢，可以通过分页懒拉取改善。

### 3、**如何保证消息的时序性**

方案：

- Id通过借鉴微信号段+跳跃的方式保证趋势递增
- 聊借鉴数据库设计，单点序列化同步到其他节点保证多机时序
- 群聊消息使用单点序列化保证各个发送者的消息相对时序

![如何保证IM实时消息的“时序性”与“一致性”？_10.jpg](https://tva1.sinaimg.cn/large/0081Kckwly1gm8m1ge2ksj30j707gt96.jpg)

优化：

- 利用服务器单点序列化时序，可能出现服务端收到消息的时序，与发出序列不一致
- 在A往B发出的消息中，加上发送方A本地的一个绝对时序，来表示接收方B的展现时序。
- 群聊消息保证一个群聊落在一个service上然后通过本地递增解决全局递增的瓶颈问题

### **4：推拉结合**

历史方案：

- 服务器在缓存集群里存储所有用户的在线状态 -> 保证状态可查
- 用户状态实时变更，任何用户登录/登出时，需要推送所有好友更新状态
- A登录时，先去数据库拉取自己的好友列表，再去缓存获取所有好友的在线状态

**“消息风暴扩散系数”**是指一个消息发出时，变成N个消息的扩散系数，这个系数与业务及数据相关，一定程度上它的大小决定了技术采用推送还是拉取。

优化方案：

- **好友状态推拉结合**，首页置顶亲密、当前群聊，采用推送，否则可以采用轮询拉取的方式同步；
- **群友的状态**，由于消息风暴扩散系数过大，可以采用按需拉取，延时拉取的方式同步；
- **系统消息/开屏广告等**这种实时产生的消息，可以采用推送的方式获取消息；

### 5、好友推荐

Neo4j 图谱数据库

## 四、智慧社区

 18年初，针对我们Dubbo框架的智慧楼宇项目的单体服务显得十分笨重，需要采用微服务的形式进行架构的重新设计，当时，我阅读了*Eric Evans* 写的《领域驱动设计：软件核心复杂性应对之道》和*Martin* *fowler*的《微服务架构：*Microservice*》两本重量级书籍，书中了解到转型微服务的重要原因之一就是利用**分治的思想**减少系统的复杂性，是一种针对**复问题的宏观设计**，来应对系统后来规模越来越大，维护越来越困难的问题。然而，拆分成微服务以后，并**不意味着每个微服务都是各自独立地运行**，而是彼此协作地组织在一起。这就好像一个团队，**规模越大越需要一些方法来组织**，这正是我们需要DDD模型为我们的架构设计提供理论并实践的方法。

 当时每次版本更新迭代动辄十几个微服务同时修改，有时一个简单的数据库字段变更，也需要同时变更多个微服务，引起了团队的反思：微服务化看上去并没有减少我们的工作量。《企业架构设计》中对于微服务的定义是**小而专**，但在起初的设计时，我们只片面的**理解了小却忽视了专**，此时我们才意识到拆分的关键是要保证微服务内高内聚，微服务间低耦合。

### **物联网架构**

> 物联网是互联网的**外延**。将用户端**延伸**和扩展到物与人的连接。物联网模式中，所有**物品与网络连接**，并进行通信和场景联动。互联网通过**电脑、移动终端**等设备将参与者联系起来，形成的一种全新的**信息互换方式**

#### DCM系统架构

- **设备感知层**（Device）：利用射频识别、二维码、传感器等技术进行数据采集
- **网络传输层**（Connect）：依托通信网络和协议，实现可信的信息交互和共享
- **应用控制层**（Manage）：分析和处理海量数据和信息，实现智能化的决策和控制



#### **三要素**

- **设备联网**：通过不同的网络协议和通信标准，实现设备与控制端的连接
- **云端分析**：提供监控、存储、分析等数据服务，以及保障客户的业务数据安全
- **云边协同**：云端接受设备上报数据，下发设备管控指令

#### 云 / 边 / 端协同

**云端计算**、**终端计算**和**边缘计算**是一个协同的系统，根据用户场景、资源约束程度、业务实时性等进行动态调 配，形成可靠、低成本的应用方案。从过去几年的发展积累来看，AI  已在物联网多个层面进行融合，比我们合作的海康威视、旷视宇视、商汤科技等纷纷发布了物联网AI相关平台和产品，和移动和小区进行了紧密的融合。



#### 物联网平台接入



向下连接海量设备，支撑设备**数据采集上云**；

向上通过调用**云端API**将指令下发至设备端，实现**远程控制**。

**上行数据链路**

- 设备建立**MQTT**长连接，上报数据（发布Topic和Payload）到物联网平台
- 物联网平台通过**配置**规则，通过**RocketMQ**、**AMQP**等队列转发到业务平台

**下行指令链路**

- 业务服务器基于**HTTPS**协议调用的API接口，发布Topic指令到物联网平台。
- 物联网平台通过**MQTT**协议，使用发布（指定Topic和Payload）到**设备端**。

#### 门锁接入

**WIFI门锁**：**非保活** 平常处于断电休眠状态，需要**MCU** **唤醒**才能传输和发送数据

**蓝牙门锁**：**MCU串口对接**和**SDK对接**，近距离**单点登录**和远距离**网关登录**

**Zigbee门锁**：**非保活** 但是保持心跳，**MCU**对接，**Zigbee协议**控制。

**NB-Iot门锁**：可以通过**公网**连接，把门禁变成**SAAS**服务，**MCU**

| 名词     | 解释                                                         |
| -------- | ------------------------------------------------------------ |
| **SaaS** | **Software-as-a-Service** ，提供给客户的服务是运营商运行在云计算基础设施上的应用程序。**用户可以在各种设备上通过客户端界面访问应用**，例如计算机浏览器。用户不需要管理或控制任何云计算基础设施，包括网络、服务器、操作系统、存储等资源，一切由 SaaS 提供商管理和运维。 |
| **PaaS** | **Platform-as-a-Service**，表示平台即服务理念，客户不需要管理或控制底层的云基础设施，包括网络、服务器、操作系统、存储等，但**客户能控制部署的应用程序**，也可能控制运行应用程序的托管环境配置。 |
| **IaaS** | I**nfrastructure-as-a-Service** ，表示基础设施即服务理念，提供的服务是对所有计算基础设施的利用，包括 CPU、内存、存储、网络等其它计算资源。**用户能够部署和运行任意软件，包括操作系统和应用程序。** |

#### 各种协议

**HTTP协议（CS用户上网）**

HTTP协议是典型的CS通讯模式，由**客户端主动**发起连接，向服务器请求**XML或JSON数据**。该协议最早是为了适用web浏览器的**上网浏览场景**和设计的，目前在**PC、手机、pad**等终端上都应用广泛，但并**不适用于物联网场景**

- 由于必须由设备主动向服务器发送数据，难以主动向设备推送数据。
- 物联网场景中的**设备多样**，运算**受限的设备**，难以实现JSON数据格式的解析

**RESTAPI（松耦合调用）**

REST/HTTP主要为了**简化**互联网中的系统架构，**快速实现**客户端和服务器之间交互的**松耦合**，降低了客户端和服务器之间的**交互延迟**。因此适合在物联网的应用层面，通过REST**开放**物联网中资源，实现服务被其他应用所调用。

**CoAP协议（无线传感）**

> 简化了HTTP协议的**RESTful API**，它适用于在**资源受限**的通信的IP网络。

**MQTT协议（低带宽）**

> MQTT协议采用**发布/订阅**模式，物联网终端都通过TCP连接到云端，云端通过主题的方式管理各个设备关注的通讯内容，**负责**将设备与设备之间**消息的转发**

适用范围：在低带宽、不可靠的集中**星型网络架构**（hub-and-spoke），不适用设备与设备之间通信，设备**控制能力弱**，另外**实时性较差**，一般都在**秒级**。协议要**足够轻量**，方便嵌入式设备去快速地解析和响应。具备**足够的灵活性**，使其足以为 IoT 设备和服务的多样化提供支持。应该设计为**异步消息协议**，这么做是因为大多数 IoT 设备的网络延迟很可能非常不稳定，若使用同步消息协议，IoT 设备需要等待服务器的响应，必须是**双向通信**，服务器和客户端应该可以互相发送消息。

**AMQP协议（互操作性）**

> 用于业务系统例如PLM，ERP，MES等进行数据交换。

　　适用范围：最早应用于金融系统之间的交易消息传递，在物联网应用中，主要适用于移动手持设备与后台数据中心的通信和分析。

**XMPP协议（即时通信）**

> 开源形式组织产生的网络即时通信协议。被IETF国际标准组织完成了标准化工作

　　适用范围：**即时通信**的应用程序，还能用在**协同工具**、游戏等。

 XMPP在通讯的业务流程上是更适合物联网系统的，开发者不用花太多心思去解决设备通讯时的业务通讯流程，相对开发成本会更低。但是HTTP协议中的安全性以及计算资源消耗的硬伤并没有得到本质的解决。

**JMS （Java消息服务）**

 Java消息服务（Java Message Service）应用程序接口，是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。

**Zigbee协议**

 低功耗，它保持IEEE 802.15.4（2003）标准

### IOT流量洪峰

智慧社区IOT领域，不管是嵌入式芯片还是应用服务器都需要传递消息，常见上行的消息有：**人脸识别开门、烟感雾感告警**、共享充电桩充电，下行的**广告下发、NB门禁开门指令、**超级门板显示等，由于物联网设备时不时会**故障和断网导致大量的流量洪峰**，传统消息队列需要针对性优化。

- **上下行拆分**

  上行消息特征：并发量**高**、可靠性和**时延性要求低**

  下行消息特征：并发量**低**、控制指令的**成功率要求高**

- **海量Topic下性能**

  **Kafka**海量Topic性能会**急剧下降**，Zookeeper协调也有瓶颈

  **多泳道消息队列**可以实现IoT消息队列的故障**隔离**

- **实时消息优先处理**

  NB门禁实时产生的开门指令必须**第一优先级处理**，堆积的消息降级

  设计成**无序、不持久化**的，并与传统的FIFO队列隔离

- **连接、计算、存储分离**

  Broker只做**流转分发**，实现**无状态**和**水平扩展**

  计算交给**Flink**，存储交给nosqlDB，实现**高吞吐写**

- **消息策略-推拉结合**

  MQTT针对电池类物联网设备，AMQP针对安全性较高的门禁设备

  消费端离线时存到queue，在线时将**实时消息和从queue中拉取的消息**一起推送

![img](https://tva1.sinaimg.cn/large/008eGmZEly1goboitd4h2j30u00ciq3i.jpg)

**如果解决海量Topic**

 首先要做的就是分区、分组等水平拆分的方式，接下来考虑单实例如何处理更多Topic，传统消息队列在海量Topic下顺序写会退化成随机写，性能大幅下降

- **人工Sharding**：部署多个Kafka集群，通过不同mq连接来隔离

- **合并Topic**，客户端封装subTopic。比如一个服务的N个统计项，会消费到无关消息

   基于这个思路，使用**Kafka Streams**或者**Hbase列**存储来聚合

针对单个Topic海量订阅的问题，**可以在上层封装广播组件来协调批量发送**

![img](https://tva1.sinaimg.cn/large/008eGmZEly1gobohzda8fj30u00cgaax.jpg)

### 社区直播带货

> 使用**端 / 边 / 云**三级架构，客户端加密传输，边缘节点转发、云侧转码并持久化

#### **产品的背景**

> 上线时间，从调研到正式上线用了 3个月时间，上线后一个月内就要经历双十二挑战。在这么紧的上线时间要求下，需要用到公司提供的所有优势，包括**cdn网络，直播牌照**等

#### 面临的挑战

- 直播数据是**实时**生成的，所有不能够进行**预缓存**
- 直播随时会发生，举办热点活动，相关服务器资源需要**动态分配**
- 直播的延迟对于用户体验影响很大，需要控制在**秒级**
- 直播sdk是内嵌在社区应用里的，整体要求不能超过5M

#### 协议的比较

| 协议            | 上线时间 | 网络兼容 | 端对端延迟 | 应用大小 | 问题                                                 |
| --------------- | -------- | -------- | ---------- | -------- | ---------------------------------------------------- |
| WebRTC          |          | ✗        |            |          | Webrtc 基于 UDP，和社区应用的网络架构不兼容          |
| HTTP Upload     |          |          | ✗          |          | 会导致网络高延迟                                     |
| Custom Protocol | ✗        |          |            |          | 工程师需要实现自己的客户端与服务端的库，无法按时上线 |
| Proprietary     |          |          |            | ✗        | 协议就需要几兆的空间，超出额度                       |
| RTMPS           | ✔        | ✔        | ✔          | ✔        | TCP实时传输消息协议，更安全更可靠                    |

#### 整体流程

**RTMPS**：基于TCP实时传输消息协议，更安全更可靠

**MPEG-DASH**：是一种基于HTTP协议自适应比特率流媒体技术，应对复杂的环境

![image-20210125145103417](https://i.loli.net/2021/01/25/zjwC7B8fdcpDytA.png)

1. 直播端使用 **RTMPS** 协议发送直播数据到**边缘节点**（POP）

2. POP 使用**RTMP**发送数据到数据中心（DC）

3. DC 将数据编码成**不同的清晰度**并进行持久化存储

   **云端转码**主要有**两种分辨率**400x400 和 720x720.

4. 播放端通过 **MPEG-DASH** / RTMPS 协议接收直播数据

   如果用户网络不好**[MPEG-DASH](https://www.cloudflare.com/zh-cn/learning/video/what-is-mpeg-dash/)**会自动转换成低分辨率

#### **直播流程**



1. 直播端使用 **RTMPS** 协议发送直播流数据到 POP 内的就近的代理服务器
2. 代理服务器**转发**直播流数据到数据中心的网关服务器（**443转80**）
3. 网关服务器使用**直播 id 的一致性哈希算法**发送直播数据到指定的编码服务器
4. 编码服务器有几项职责：
   - 4.1 **验证直播数据**的格式是否正确。
   - 4.2 **关联**直播 id 以及编码服务器第一映射，保证客户端即使连接中断或者服务器扩容时，在**重新连接**的时候依然能够连接到相同的编码服务器
   - 4.3 使用直播数据**编码成不同解析度**的输出数据
   - 4.4 使用 **DASH** 协议输出数据并**持久化**存储

#### 播放流程



1. 播放端使用 HTTP **DASH** 协议向 POP 拉取直播数据
2. POP 里面的代理服务器会检查数据是否已经在 POP 的**缓存**内。如果是的话，缓存会返回数据给播放端，否则，代理服务器会向 DC 拉取直播数据
3. DC 内的代理服务器会检查数据是否在 DC 的缓存内，如果是的话，缓存会返回数据给 POP，并更新 POP 的缓存，再返回给播放端。不是的话，代理服务器会使用一致性哈希算法向对应的编码服务器请求数据，并更新 DC 的缓存，返回到 POP，再返回到播放端。

**收获**

1. 项目的成功不，代码只是内功，考虑适配不同的网络、利用可利用的资源
2. 惊群效应在热点服务器以及许多组件中都可能发生
3. 开发大型项目需要对**吞吐量和时延**、**安全和性能**做出妥协
4. 保证架构的灵活度和可扩展性，为内存、服务器、带宽耗尽做好规划

### **直播高可用方案**

**网络可靠性**：

- 根据**网络连接速度**来自动调整视频质量
- 使用**短时间的数据缓存**来解决直播端不稳定，瞬间断线的问题
- 根据**网络质量自动降级**为音频直播以及播放

**惊群效应：**

- 当多个播放端向同一个 POP 请求直播数据的时候，如果数据不在缓存中
- 这时候只有一个请求 A 会到 DC 中请求数据，其他请求会等待结果
- 但是如果请求 A 超时没有返回数据的话，所有请求会一起向 DC 访问数据
- 这时候就会加大 DC 的压力，触发惊群效应
- 解决这个问题的方法就是通过**实际的情况**来调整请求超时的时间。这个时间如果太长的话会带来直播的延迟，太短的话会经常触发惊群效应（**每个时间窗口只允许触发一次**，设置允许最大回源数量）

### **性能优化方案**

![img](https://tva1.sinaimg.cn/large/008eGmZEly1gobogaxxjkj304v0e7t93.jpg)

**数据库优化：** 数据库是最容易成为瓶颈的组件，考虑从 SQL 优化或者数据库本身去提高它的性能。如果瓶颈依然存在，则会考虑分库分表将数据打散，如果这样也没能解决问题，则可能会选择缓存组件进行优化

**集群最优：**存储节点的问题解决后，计算节点也有可能发生问题。一个集群系统如果获得了水平扩容的能力，就会给下层的优化提供非常大的时间空间，由最初的 3 个节点，扩容到最后的 200 多个节点，但由于人力问题，服务又没有什么新的需求，下层的优化就一直被搁置着。

**硬件升级：**水平扩容不总是有效的，原因在于单节点的计算量比较集中，或者 JVM 对内存的使用超出了宿主机的承载范围。在动手进行代码优化之前，我们会对节点的硬件配置进行升级。

**代码优化**：代码优化是提高性能最有效的方式，但需要收集一些数据，这个过程可能是服务治理，也有可能是代码流程优化。比如JavaAgent 技术，会无侵入的收集一些 profile 信息，供我们进行决策。

**并行优化：**并行优化是针对速度慢的接口进行并行调用。所以我们通常使用 ContDownLatch 对需要获取的数据进行并行处理，效果非常不错，比如在 200ms 内返回对 50 个耗时 100ms 的下层接口的调用。

**JVM 优化**： JVM 发生问题时，优化会获得巨大的性能提升。但在 JVM 不发生问题时，它的优化效果有限。但在代码优化、并行优化、JVM 优化的过程中，JVM 的知识却起到了关键性的作用

**操作系统优化：**操作系统优化是解决问题的杀手锏，比如像 HugePage、SWAP、“CPU 亲和性”这种比较底层的优化。但就计算节点来说，对操作系统进行优化并不是很常见。运维在背后会做一些诸如文件句柄的调整、网络参数的修改，这对于我们来说就已经够用了

### 流量回放自动化测试

> 系统级的重构，测试回归的工作量至少都是以月为单位，对于人力的消耗巨大。一种应对方案是，先不改造，到系统实在扛不住了再想办法。另一种应对方案是，先暂停需求，全力进行改造。但在实际工作场景中，上述应对策略往往很难实现。

场景：

1、读服务均是查询，它是无状态的。

2、不管是架构升级还是日常需求，读服务对外接口的出入参格式是没有变化的



- **日志收集**，主要作用是收集被测系统的真实用户请求，基于一定规则处理后作为系统用例；

  Spring 里的 Interceptor 、Servlet 里的 Filter 过滤器，对所有请求的入参和出参进行记录，并通过 MQ 发送出去。（注意错峰、过滤写、去重等）

- 数据回放是基于收集的用例，对被测系统进行数据回放，发起自动化测试回归；

  **离线回放：**只调用新服务，将返回的数据和日志里的出参进行比较，**日志比较大**

  **实时回放：**去实时调用线上系统和被测系统，并存储实时返回回放的结果信息，**线上有负担**

  **并行回放：**新版本不即时上线，每次调用老版本接口时概率实时回放新版本接口，**耗时间周期**

- **差异对比**，通过差异对比自动发现与预期不一致的用例，进而确定 Bug。

  采用文本对比，可以直观地看到哪个字段数据有差异，从而更快定位到问题。正常情况下，只要存在差异的数据，均可认为是 Bug，是需要进行修复的。

**方法论**

**Discovery**

 考虑企业战略，分析客户需求，制定产品目标

 由外到内：竞争对手的方案，为什么做，以后怎么发展，如何去优化。

 自上而下：基于公司的战略，考虑自身能力和所处环境。

 自下而上：从资源、历史问题、优先级出发，形成一套可行性施方法。

**Define**

 基于收集的信息，综合跨业务线的抽象能力和服务，先做什么后做什么，怎么做

 设计新的架构，重点设计解决痛点问题。

 拆分业务领域，重点划分工作临界上下文。

**Design**

 详细的业务设计，功能设计，交付计划，考核计划

 产品愿景，产品形态，相关竞品方案对比，价值、优势、收益

 梳理业务范围，要知道电商领域四大流（信息流、商流、资金流、物流）

 MVP最小可用比，让客户和老大看到结果，最后通编写story把故事编圆

**Delivery**

 交付阶段，根据反馈及时调整中台战略，减少损失和增大收益

 合理制定每个阶段的绩效考核目标：

 40%稳定+25%业务创新+20%服务接入+15%用户满意度

# **七、架构设计**

## 1、社区系统的架构



**系统拆分**

 通过DDD领域模型，对服务进行拆分，将一个系统拆分为多个子系统，做成SpringCloud的微服务。微服务设计时要尽可能做到少扇出，多扇入，根据服务器的承载，进行客户端负载均衡，通过对核心服务的上游服务进行限流和降级改造。

 一个服务的代码不要太多，1 万行左右，两三万撑死了吧。

 大部分的系统，是要进行**多轮拆分**的，第一次拆分，可能就是将以前的多个模块该拆分开来了，比如说将电商系统拆分成**订单系统、商品系统、采购系统、仓储系统、用户系统**等等吧。

 但是后面可能每个系统又变得越来越复杂了，比如说采购系统里面又分成了**供应商管理系统、采购单管理系统**，订单系统又拆分成了**购物车系统、价格系统、订单管理**系统。

**CDN、Nginx静态缓存、JVM缓存**

 利用Java的模板thymeleaf可以将页面和数据动态渲染好，然后通过Nginx直接返回。动态数据可以从redis中获取。其中redis里的数据由一个缓存服务来进行消费指定的变更服务。

 商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。目前高峰期每秒就是 3500 左右的请求量。

**缓存**

Redis cluster，10 台机器，5主5从，5 个节点对外提供读写服务，**每个节点的读写高峰 QPS** 可能可以达到每秒 5 万，**5 台机器最多是 25 万读写**请求每秒。

 **32G 内存+ 8 核 CPU + 1T** 磁盘，但是分配给 **Redis 进程的是 10g 内存**，一般线上生产环境，Redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。

 因为每个主实例都挂了一个从实例，所以是**高可用**的，任何一个主实例宕机，都会自动故障迁移，Redis 从实例**会自动变成主实例**继续提供读写服务。

**MQ**

 可以通过消息队列对微服务系统进行[解耦](https://www.zybuluo.com/mdeditor#1、拆分微服务)，异步调用的更适合微服务的扩展

 同时可以应对秒杀活动中[应对高并发写请求](# 6、应对高并发的写请求)，比如kafka在毫秒延迟基础上可以实现10w级吞吐量

 针对[IOT流量洪峰](https://www.zybuluo.com/mdeditor#IOT流量洪峰)做了一些特殊的优化，保证消息的及时性

 同时可以使用消息队列保证分布式系统[最终一致性](https://www.zybuluo.com/mdeditor#7、如何保证数据一致性)

**分库分表**

 分库分表，可能到了最后数据库层面还是免不了抗高并发的要求，好吧，那么就 将一个数据库拆分为多个库，多个库来扛更高的并发；然后将一个表拆分为多个 表，每个表的数据量保持少一点，提高 sql 跑的性能。**在通讯录、订单和商城商品模块超过千万级别都应及时考虑分表分库**

**读写分离**

 读写分离，这个就是说大部分时候数据库可能也是读多写少，没必要所有请求都 集中在一个库上吧，可以搞个主从架构，主库写入，从库读取，搞一个读写分离。 读流量太多的时候，还可以加更多的从库。比如**统计监控类的微服务**通过读写分离，只需访问从库就可以完成统计，例如ES

**ElasticSearch**

 Elasticsearch，简称 es。es 是分布式的，可以随便扩容，分布式天然就可以支撑高并发，因为动不动就可以扩容加机器来扛更高的并发。那么一些比较简单的**查询、统计类**的操作，比如**运营平台上**的各地市的汇聚统计，还有一些**全文搜索类**的操作，比如**通讯录和订单**的查询。

## 2、商城系统-亿级商品如何存储

基于 Hash 取模、一致性 Hash 实现分库分表

高并发读可以通过多级缓存应对

大促销热key读的问题通过redis集群+本地缓存+限流+key加随机值分布在多个实例中

高并发写的问题通过**基于 Hash 取模、一致性 Hash 实现分库分表**均匀落盘

业务分配不均导致的**热key**读写问题，可以根据业务场景进行range分片，将热点范围下的子key打散

具体实现：预先设定主键的生成规则，根据规则进行数据的分片路由，但这种方式会侵入商品各条线主数据的业务规则，更好的方式是基于**分片元数据服务器**（即每次访问分片前先询问分片元服务器在路由到实际分片）不过会带来复杂性，比如保证元数据服务器的**一致性**和可用性。

## 3、对账系统-分布式事务一致性

> 尽量避免分布式事务，单进程用数据库事务，跨进程用消息队列

主流实现分布式系统事务一致性的方案：

1. **最终一致性**：也就是基于 MQ 的可靠消息投递的机制，
2. 基于重试加确认的的**最大努力通知方案**。

理论上也可以使用（2PC两阶段提交、3PC三阶段提交、TCC短事务、SAGA长事务方案），但是这些方案工业上落地代价很大，不适合互联网的业界场景。针对金融支付等需要强一致性的场景可以通过前两种方案实现。（**展开说的话参考分布式事务**）

![image-20210321212516364](https://tva1.sinaimg.cn/large/008eGmZEly1goruh4oifej30xq0auq7p.jpg)

本地数据库事务原理：**undo log**（原子性） + **redo log**（持久性） + **数据库锁**（原子性&隔离性） + **MVCC**（隔离性）

分布式事务原理：**全局事务协调器（原子性）** + 全局锁（隔离性） + **DB本地事务（原子性、持久性）**

一、我们公司账单系统和第三方支付系统对账时，就采用“**自研补偿/MQ方案 + 人工介入**”方式

落地的话：方案最“轻”，性能损失最少。可掌控性好，简单易懂，易维护。
考虑到分布式事务问题是小概率事件，留有补救余地就行，性能的损失可是实打实的反应在线上每一个请求上

二、也了解到业界比如阿里成熟**Seata AT**模式，平均性能会降低35%以上

我觉得不是特殊的场景不推荐

三、RocketMQ事务消息

听起来挺好挺简单的方案，但它比较挑业务场景，同步性强的处理链路不适合。
【重要】要求下游MQ消费方一定能成功消费消息。否则转人工介入处理。
【重要】千万记得实现幂等性。

## 4、用户系统-多线程数据割接

由于项目需要进行数据割接，保证用户多平台使用用户感知的一致，将广东项目的几百万用户及业务数据按照一定的逻辑灌到社区云平台上，由于依赖了第三方统一认证和省侧crm系统，按照之前系统内割接的方法，通过数据库将用户的唯一标识查出来然后使用多线程向省侧crm系统获取结果。

但是测试的过程中，发现每个线程请求的数据发生了错乱，导致每个请求处理的数据有重复，于是立即停止了脚本，当时怀疑是多线程对资源并发访问导致的，于是把ArrayList  改成了CopyOnWriteArrayList，但是折腾了一晚上，不管怎么修改，线程之间一直有重复数据，叫了一起加班的同事也没看出问题来，和同事估算了一下不使用多线程，大概30-40个小时能跑完，想了下也能接受，本来已经准备放弃了。

不过回到家，我还是用多线程仔细单步模拟了下，整个处理的过程，发现在起线程的时候，有些子线程并没有把分配给他的全部id的list处理完，导致最终状态没更新，新线程又去执行了一遍，然后我尝试通过修改在线程外深拷贝一个List再作为参数传入到子线程里，（后续clear的时候也是clear老的List）果然，整个测试过程中再也没出现过重复处理的情况。

事后，我也深究了下原因：

```
if(arrayBuffer.length == 99) { val asList = arrayBuffer.toList exec.execute ( openIdInsertMethod(asList) ) arrayBuffer.clear}
```

在一个线程中开启另外一个新线程，则新开线程称为该线程的子线程，子线程初始优先级与父线程相同。不过主线程先启动占用了cpu资源，因此主线程总是优于子线程。然而，即使设置了优先级，也无法保障线程的执行次序。只不过，优先级高的线程获取CPU资源的概率较大，优先级低的并非没机会执行。

所以主线程上的clear操作有可能先执行，那么子线程中未处理完的数据就变成一个空的数组，所以就出现了多个线程出现了重复数据的原因，所以我们要保证的是子线程每次执行完后再进行clear即可。而不是一开始定位的保证ArrayList的安全性。所以将赋值(buffer->list)操作放在外面执行后，多线程数据就正常了。

## 5、秒杀系统场景设计

[见秒杀项目方案设计](# 二、秒杀项目)

## **6、统计系统-海量计数**

**中小规模的计数服务**（万级）

最常见的计数方案是采用缓存 + DB 的存储方案。当计数变更时，先变更计数 DB，计数加 1，然后再变更计数缓存，修改计数存储的 Memcached 或  Redis。这种方案比较通用且成熟，但在高并发访问场景，支持不够友好。在互联网社交系统中，有些业务的计数变更特别频繁，比如微博 feed  的阅读数，计数的变更次数和访问次数相当，每秒十万到百万级以上的更新量，如果用 DB 存储，会给 DB 带来巨大的压力，DB  就会成为整个计数服务的瓶颈所在。即便采用聚合延迟更新 DB 的方案，由于总量特别大，同时请求均衡分散在大量不同的业务端，巨大的写压力仍然是 DB 的不可承受之重。

**大型互联网场景**（百万级）

直接把计数全部存储在 Redis 中，通过 hash 分拆的方式，可以大幅提升计数服务在 Redis 集群的写性能，通过主从复制，在 master  后挂载多个从库，利用读写分离，可以大幅提升计数服务在 Redis 集群的读性能。而且 Redis 有持久化机制，不会丢数据

一方面 Redis 作为通用型存储来存储计数，内存存储效率低。以存储一个 key 为 long 型 id、value 为 4  字节的计数为例，Redis 至少需要 65 个字节左右，不同版本略有差异。但这个计数理论只需要占用 12 个字节即可。内存有效负荷只有  12/65=18.5%。如果再考虑一个 long 型 id 需要存 4 个不同类型的 4 字节计数，内存有效负荷只有  (8+16)/(65*4)= 9.2%。

另一方面，Redis 所有数据均存在内存，单存储历史千亿级记录，单份数据拷贝需要 10T 以上，要考虑核心业务上 1 主 3 从，需要 40T 以上的内存，再考虑多 IDC 部署，轻松占用上百 T 内存。就按单机 100G 内存来算，计数服务就要占用上千台大内存服务器。存储成本太高。

**微博、微信、抖音**（亿级）

定制数据结构，共享key 紧凑存储，提升计数有效负荷率

超过阈值后数据保存到SSD硬盘，内存里存索引

冷key从SSD硬盘中读取后，放入到LRU队列中

自定义主从复制的方式，海量冷数据异步多线程并发复制

## 7、系统设计 - 微软

### **1、需求收集**

确认**使用的对象**（ToC：高并发，ToB：高可用）

**系统的服务场景**（**即时通信**：低延迟，**游戏**：高性能，**购物**：秒杀-一致性）

**用户量级**（**万级**：双机、**百万**：集群、**亿级**：弹性分布式、容器化编排架构）

**百万读**：3主6从，**每个节点的读写高峰 QPS** 可能可以达到每秒 5 万，可以实现15万，30万读性能

**亿级读**，通过CDN、静态缓存、JVM缓存等多级缓存来提高读并发

**百万写**，通过消息队列削峰填谷，通过hash分拆，水平扩展分布式缓存

**亿级写**，redis可以定制数据结构、SSD+内存LRU、冷数据异步多线程复制

持久化，（Mysql）承受量约为 1K的QPS，读写分离提升**读并发**，分库分表提升**写并发**

### **2、顶层设计**

核心功能包括什么：

写功能：发送微博

读功能：热点资讯

交互：点赞、关注

### **3、系统核心指标**

- 系统

  性能

  和

  延迟

  - 边缘计算 | 动静分离 | 缓存 | 多线程 |

- 可扩展性

  和

  吞吐量

  - 负载均衡 | 水平扩展 | 垂直扩展 | 异步 | 批处理 | 读写分离

- 可用性

  和

  一致性

  - 主从复制 | 哨兵模式 | 集群 | 分布式事务

### 4、数据存储

键值存储 : Redis ( 热点资讯 )

文档存储 : MongoDB ( 微博文档分类)

分词倒排：Elasticsearch（搜索）

列型存储：Hbase、BigTable（大数据）

图形存储：Neo4j （社交及推荐）

多媒体：FastDfs（图文视频微博）

## 7、如何设计一个微博

**实现哪些功能：**

筛选出核心功能（Post a Tweet，Timeline，News Feed，Follow/Unfollow a user，Register/Login）

**承担多大QPS：**

QPS = 100，那么用我的笔记本作Web服务器就好了

QPS = 1K，一台好点的Web 服务器也能应付，需要考虑单点故障；

QPS = 1m，则需要建设一个1000台Web服务器的集群，考虑动态扩容、负载分担、故障转移

一台 SQL Database （Mysql）承受量约为 1K的QPS；

一台 NoSQL Database (Redis) 约承受量是 20k 的 QPS；

一台 NoSQL Database (Memcache) 约承受量是 200k 的 QPS；

**微服务战略拆分**



**针对不同服务选择不同存储**

[v2-13cab4d5f56e3ecb682c351c0eb4a24b_1440w.jpg?source=1940ef5c未知大小](https://pic1.zhimg.com/80/v2-13cab4d5f56e3ecb682c351c0eb4a24b_1440w.jpg?source=1940ef5c)

**设计数据表的结构**

![img](https://tva1.sinaimg.cn/large/008eGmZEly1goruu4homyj31400ht405.jpg)

基本差不多就形成了一个解决方案，但是并不是完美的，仍然需要小步快跑的不断的针对**消息队列、缓存、分布式事务、分表分库、大数据、监控、可伸缩**方面进行优化

# 八、领域模型落地

### 1、拆分微服务

>  微服务内高内聚，微服务间低耦合

**微服务内高内聚**即单一职责原则

 每个微服务中的代码变化都是同一类原因。因这类原因而需要变更的代码都在这个微服务中，与其他微服务无关，那么就可以将代码修改的范围缩小到这个微服务内。把这个微服务修改好了，独立修改、独立发布，该需求就实现了。这样，微服务的优势才能发挥出来。

**微服务间低耦合**开放封闭原则

 就是说在微服务实现自身业务的过程中，如果需要执行的某些过程不是自己的职责，就应当将这些过程交给其他微服务去实现，你只需要对它的接口进行调用。这样，微服务之间的调用就实现了解耦。

 **领域建模**就是将一个系统划分成了多个子域，每个子域都是一个独立的业务场景，每个子域的边界就是“**限界上下文**”。该业务场景会涉及许多领域对象，但**分析建模**始终需要围绕着业务场景的上下文进行。

 **领域事件通知机制**最有效的方式就是通过消息队列，实现领域事件在微服务间的通知。

> “核心通讯录”微服务只负责发送变更消息到消息队列，不管谁会接收并处理这些消息；
>
> “门禁管理”微服务只负责接收照片变更消息，不管谁发送的这个消息。

### 2、关联微服务

1. 按照**限界上下文**进行微服务的拆分，将领域模型**划分到多个问题子域**

2. 基于**充血模型**与**贫血模型**设计各个微服务的业务领域层（Service、Entity、Value）

3. 通过**领域事件通知机制**和**微服务调用**的推拉结合，将各个子域进行解耦关联

   - **核心**：
   - 通讯录 | 短信 | 推送通知 | 支付 | 文件服务
   - **智慧通行**

   > 解决物业多品牌、多系统应用造成的**信息孤岛**，**数据混乱**的问题

   - 人脸门禁 | 可视对讲 | 电梯梯控 | 停车系统 | 访客预约
   - **安全社区**

   > 通过**图像视频识别**、**传感数据采集**，实现**报警联动**和**风险预警**

   - 视频监控 | 周界报警 | 高空抛物 | 跨域追踪
   - **全屋智能**

   > 围绕业主需求，逐步引入社区医疗、社区养老、**社区团购**、**社区家政**等服务

   - 超级面板 | 无线门锁 | 烟感雾感
   - **增值服务**

   > 实现跨品牌的产品体验，支持基于**matrix引擎**的智能生活场景裂变能力

   - 智能充电 | 云广播 | 出入提醒 | 定向投放

### **3、微服务的落地**

>  通过合理的微服务设计，尽量让每次的需求变更都交给某个小团队独立完成，让需求变更落到某个微服务上进行变更。唯有这样，每次变更只需独立地修改这个微服务，独立打包、独立升级，新需求独立实现，才能发挥微服务的优势。

- **数据隔离：**数据库中用户信息表的读写只有**通讯录**微服务。当其他微服务需要读写用户信息时，就不能直接读取用户信息表，而是通过 API 接口去调用**通讯录**微服务。
- **接口复用：**因此，当多个团队向你提需求时，必须要对这些接口进行规划，通过复用**尽可能少的接口满足他们的需求；**当有新的接口提出时，要尽量通过现有接口解决问题。
- **向前兼容：**当调用方需要接口变更时怎么办？变更现有接口应当尽可能向前兼容，即接口的名称与参数都不变，只是在内部增加新的功能。**宁愿增加一个新的接口也最好不要去变更原有的接口。**
- **本地调用：**在**访客申请**微服务的本地，增加一个**查询用户Service**的 feign 接口。这样，**访客申请Service**就像本地调用一样调用**查询用户Service**，再通过 feign 接口实现远程调用。这种**防腐层**的设计，可以隔离当前微服务以外的其他微服务拆分变更导致的接口的失效的影响。
- **数据库去中心化：**
  - 微服务中**通讯录服务**与**健康码服务**分别对应的**用户库与权限库**，它们的共同特点是数据量小但频繁读取，可以选用小型的 MySQL 数据库并在前面架设 Redis 来提高查询性能；
  - 微服务中**访客通行**与**生活缴费**分别对应的**通行记录库、订单库**，其特点是数据量大并且高并发写，选用一个数据库显然扛不住这样的压力，因此可以选用了 TiDB 这样的 NewSQL 数据库进行分布式存储，将数据压力分散到多个数据节点中，从而解决 I/O 瓶颈；
  - 微服务中**数据分析**与**通讯录查询**这样的查询分析业务，则选用 **NoSQL 数据库**或**大数据平台**，通过读写分离将生产库上的数据同步过来进行分布式存储，然后宽表一系列的预处理，应对海量历史数据的决策分析与秒级查询。（ NoSQL 为空的字段是不占用空间的，因此字段再多都不影响查询性能）

### 4、领域模型的意义

 **贫血模型、充血模型、策略模式、装饰者模式**只是DDD实现的方式，而DDD的真谛是**领域建模**。

 做事不能仅凭一腔热血，一定要符合自然规律。其实软件的设计开发过程也是这样。对业务理解不深刻全局架构设计往往是过度设计，这时候**应该抓主要流程**，开始领建模。

- 接着，每次添加新功能的时候，一方面要满足当前的需求，另一方面业务相关的**领域建模设计**刚刚满足需求，从而使设计最简化、代码最少。
- 这样的设计过程叫**小步快跑**。采用小步快跑的设计方法，一开始不用思考那么多问题，从简单问题开始逐步深入。**领域模型**就像小树一样一点儿一点儿成长，最后完成所有的功能。

> 保持软件设计不退化的关键在于每次需求变更的设计，只有保证每次需求变更时做出正确的设计，才能保证软件以一种良性循环的方式不断维护下去。

 有没有一种方法，让我们在第十次变更、第二十次变更、第三十次变更时，依然能够找到正确的设计呢？有，那就是**领域驱动设计**

 那么在每次需求变更时，将变更还原到真实世界中，看看真实世界是什么样子的，根据真实世界进行变更。

### 5、战略建模



### **6、相关名词**

**领域和子域（Domain/Subdomain）**

 在**上下文地图**构建的领域中，对应模块，使用**限界上下文**划分领域，对应微服务

**限界上下文（Bounded Context）**

 在一个领域/子域中，有概念上的领域边界，任何**领域对象**在该边界内部的有不依赖外部的确切含义。

**领域对象**

 服务、实体与值对象是领域驱动设计的领域对象，可以通过**贫血模型**和**充血模型**转换为程序设计

**实体和值对象**

 通过一个**唯一标识字段来区分**真实世界中的每一个个体的领域对象，称为实体。真实世界中那些**一成不变的**、本质性的事物的领域对象，称为值对象。 **可变性**是实体的特点，而**不变性**则是值对象的本质。

**贫血模型与充血模型**

 POJO对象中只保存get/set方法，没有任何业务逻辑，这样的设计被称为**贫血模型**

 **充血模型**是封装和继承思想的体现，门禁设备实体中，包含特征值下发、广告下发、通行记录回调等方法，不同厂商的实体针对多态进行**聚合**，并通过**工厂或仓库**对外提供服务。在充血模型中， Service 只干一件非常简单的事，就是直接去调用对象中的**工厂方法**生成不同产品，其他的什么都不干。

**聚合**

 聚合体现的是一种**整体与部分**的关系。正是因为有这样的关系，在操作整体的时候，整体就封装了对部分的操作。如何正确理解是否存在聚合的关系：就是当**整体不存在**时，部分就变得**没有了意义**。部分是整体的一个部分，与**整体有相同的生命周期**。

**工厂**

**通过装配，创建领域对象，是领域对象生命周期的起点。**譬如，系统要通过 ID 装载一个访客申请：

1. 表单工厂分别调用表单信息DAO、表单明细 DAO 和用户DAO 去进行查询；
2. 将得到的表单明细对象、用户对象进行装配，分别 set 到**表单信息对象**的**表单明细**与**用户属性**中；
3. 最后，表单工厂将装配好的表单对象返回给表单仓库。

**仓库**

 如果服务器是一个非常强大的服务器，那么我们不需要任何数据库。系统创建的所有领域对象都放在仓库中，当需要这些对象时，通过 ID 到仓库中去获取。

- 当客户程序通过 ID 去获取某个领域对象时，仓库会通过这个 ID 先到**缓存中进行查找**：
- 查找到了，则**直接返回**，不需要查询数据库；
- 没有找到，则通知工厂，工厂调用 DAO 去数据库中查询，然后**装配成领域对象返回给仓库**。
- 仓库在收到这个领域对象以后，在返回给客户程序的同时，将该**对象放到缓存中。**